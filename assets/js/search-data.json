{
  
    
        "post0": {
            "title": "Submitting to Image Matching Challenge 2021",
            "content": "What is Image Matching Challenge? . IMC is a benchmark and challenge for the local features (such as SIFT, SuperPoint, etc), matching methods (CNe, SuperGlue, etc.) and robust geometry estimators such as RANSAC, at CVPR 2021 Workshop on Image Matching. . I will walk you through the submission process, including writing setting-up an environment, writing a processing script and tuning matching and RANSAC for the best performance. As this is a tutorial, not a research paper, we use local feature descriptor available in kornia for this sample submission. . All the codes and scripts in this tutorial are also avilable at https://github.com/ducha-aiki/imc2021-sample-kornia-submission. Let&#39;s go! . Setting up the environment . First, let&#39;s clone the benchmark repository. . git clone https://github.com/ubc-vision/image-matching-benchmark cd image-matching-benchmark git submodule update --init --recursive . Now we need to create conda virtual environment. I assume that you have conda installed, and if not - please, follow instructions here . conda env create -f system/conda_env_dm.yml . After successfull environment creation, let&#39;s activate it . conda activate sfm . Downloading the data . IMC-2021 data consists of 3 datasets: Phototourism, PragueParks and GoogleUrban. You can download first two freely, but need to request a credentials and accept license agreement for the GoogleUrban dataset. . Latter is done by writing an email to image-matching@googlegroups.com and asking for the password. All datasets consist of two parts each: test part, which contains only images and validation part, which comes together with ground truth. Validation ground truth can (and should!) be used for hyperparameter tuning for your submission. . cd .. mkdir imc-2021-data cd imc-2021-data wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-test-public-pragueparks.tar.gz wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-validation-pragueparks.tar.gz wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-test-public-phototourism.tar.gz wget https://www.cs.ubc.ca/research/kmyi_data/imc2021-public/imc-2021-validation-phototourism.tar.gz . I am assuming that you have requsted an access to the GoogleUrban dataset, downloaded it and put in the same directory, as the rest of the data: . (sfm) mishkdmy@n33:~/dev/imc-2021-data$ ls imc-2021-test-public-googleurban.tar.gz imc-2021-validation-googleurban.tar.gz imc-2021-test-public-phototourism.tar.gz imc-2021-validation-phototourism.tar.gz imc-2021-test-public-pragueparks.tar.gz imc-2021-validation-pragueparks.tar.gz . Now let&#39;s unpack it. . for f in *.tar.gz ; do tar -xzf $f; done . Now the directory should look like this: . (sfm) mishkdmy@n33:~/dev/imc-2021-data$ ls googleurban imc-2021-validation-phototourism.tar.gz imc-2021-test-public-googleurban.tar.gz imc-2021-validation-pragueparks.tar.gz imc-2021-test-public-phototourism.tar.gz phototourism imc-2021-test-public-pragueparks.tar.gz pragueparks imc-2021-validation-googleurban.tar.gz . Extracting the features . Let&#39;s start with creating the directory for our scripts (or you can clone it from here ) . cd .. mkdir imc2021-sample-kornia-submission cd imc2021-sample-kornia-submission . Now we will create a script, which extracts AffNet-HardNet8 descriptors on top of OpenCV SIFT keypoints. . We need to install pytorch and kornia for this: . pip install torch torchvision kornia pip install kornia_moons --no-deps . Great! Now we are ready to extract the features from the images. Required imports and initializations: . import matplotlib.pyplot as plt import numpy as np import cv2 import torch import kornia as K import kornia.feature as KF from kornia_moons.feature import * device = torch.device(&#39;cpu&#39;) try: if torch.cuda.is_available(): device = torch.device(&#39;cuda&#39;) print (&quot;GPU mode&quot;) except: print (&#39;CPU mode&#39;) #device = torch.device(&#39;cpu&#39;) # SIFT (DoG) Detector sift_det = cv2.SIFT_create(8000, contrastThreshold=-10000, edgeThreshold=-10000) # HardNet8 descriptor hardnet8 = KF.HardNet8(True).eval().to(device) # Affine shape estimator affnet = KF.LAFAffNetShapeEstimator(True).eval().to(device) . Now we can define extract_features function. Feel free to modify it for your own features. . def extract_features(img_fname, detector, affine, descriptor, device, visualize=False): img = cv2.cvtColor(cv2.imread(img_fname), cv2.COLOR_BGR2RGB) if visualize: plt.imshow(img) kpts = detector.detect(img, None) # We will not train anything, so let&#39;s save time and memory by no_grad() with torch.no_grad(): timg = K.image_to_tensor(img, False).float()/255. timg = timg.to(device) timg_gray = K.rgb_to_grayscale(timg) # kornia expects keypoints in the local affine frame format. # Luckily, kornia_moons has a conversion function lafs = laf_from_opencv_SIFT_kpts(kpts, device=device) lafs_new = affine(lafs, timg_gray) if visualize: visualize_LAF(timg, lafs_new, 0) patches = KF.extract_patches_from_pyramid(timg_gray, lafs_new, 32) B, N, CH, H, W = patches.size() # Descriptor accepts standard tensor [B, CH, H, W], while patches are [B, N, CH, H, W] shape # So we need to reshape a bit :) descs = descriptor(patches.view(B * N, CH, H, W)).view(B * N, -1).detach().cpu().numpy() return kpts, descs . Let&#39;s check how it works on a single image. . img_fname = &#39;../imc-2021-data/pragueparks/wooden_lady/set_100/images/IMG_9603.MOV_frame000001.jpg&#39; kpts, descs = extract_features(img_fname, sift_det, affnet, hardnet8, device, True) . So far, so good. Now we need to convert our keypoints from OpenCV format to the benchmark format, which is numpy.array [N x dim] . def convert_kpts_to_imc(cv2_kpts): keypoints = np.array([(x.pt[0], x.pt[1]) for x in cv2_kpts ]).reshape(-1, 2) scales = np.array([12.0* x.size for x in cv2_kpts ]).reshape(-1, 1) angles = np.array([x.angle for x in cv2_kpts ]).reshape(-1, 1) responses = np.array([x.response for x in cv2_kpts]).reshape(-1, 1) return keypoints, scales, angles, responses . Now we are ready to write a script, which extracts local features for all images in the IMC-2021. The full script is accesible here . import os import h5py from tqdm import tqdm INPUT_DIR = &#39;../imc-2021-data&#39; OUT_DIR = &#39;extracted/cv2-dog-affnet-hardnet8&#39; os.makedirs(OUT_DIR, exist_ok=True) datasets = os.listdir(INPUT_DIR) for ds in datasets: ds_in_path = os.path.join(INPUT_DIR, ds) ds_out_path = os.path.join(OUT_DIR, ds) os.makedirs(ds_out_path, exist_ok=True) seqs = os.listdir(ds_in_path) for seq in seqs: if os.path.isdir(os.path.join(ds_in_path, seq, &#39;set_100&#39;)): seq_in_path = os.path.join(ds_in_path, seq, &#39;set_100&#39;, &#39;images&#39;) else: seq_in_path = os.path.join(ds_in_path, seq) seq_out_path = os.path.join(ds_out_path, seq) os.makedirs(seq_out_path, exist_ok=True) img_fnames = os.listdir(seq_in_path) num_kp = [] with h5py.File(f&#39;{seq_out_path}/keypoints.h5&#39;, &#39;w&#39;) as f_kp, h5py.File(f&#39;{seq_out_path}/descriptors.h5&#39;, &#39;w&#39;) as f_desc, h5py.File(f&#39;{seq_out_path}/scores.h5&#39;, &#39;w&#39;) as f_score, h5py.File(f&#39;{seq_out_path}/angles.h5&#39; &#39;w&#39;) as f_ang, h5py.File(f&#39;{seq_out_path}/scales.h5&#39;, &#39;w&#39;) as f_scale: for img_fname in tqdm(img_fnames): img_fname_full = os.path.join(seq_in_path, img_fname) key = os.path.splitext(os.path.basename(img_fname))[0] kpts, descs = extract_features(img_fname_full, sift_det, affnet, hardnet8, device, False) keypoints, scales, angles, responses = convert_kpts_to_imc(kpts) f_kp[key] = keypoints f_desc[key] = descs.reshape(-1, 128) f_score[key] = responses f_ang[key] = angles f_scale[key] = scales num_kp.append(len(keypoints)) print(f&#39;Finished processing &quot;{ds}/{seq}&quot; -&gt; {np.array(num_kp).mean()} features/image&#39;) . Creating config json file . In addition to features, we should submit a config file, which tells the benchmark, how the features should be matched and which RANSAC we prefer. In priciple, we can just write an arbitrary config file and submit already, but this may lead to the bad results. Let&#39;s instead generate a config file from python, so we can easily re-generate it. Why would we need this? Quite simple - to try different parameters on the validation set and only then create a final config. . First part of the config is metadata -- information about the method and authors. If your method is under review, you may want to set flag publish_anonymously to True. . metadata_dict = { &quot;publish_anonymously&quot;: False, &quot;authors&quot;: &quot;Dmytro Mishkin, Milan Pultar and kornia team&quot;, &quot;contact_email&quot;: &quot;ducha.aiki@gmail.com&quot;, &quot;method_name&quot;: &quot;CV-DoG-AffNet-HardNet8 (kornia)&quot;, &quot;method_description&quot;: r&quot;&quot;&quot;OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization and HardNet8 descriptor as implemented in kornia. Matched using the built-in matcher (bidirectional filter with the &#39;both&#39; strategy, hopefully optimal inlier and ratio test thresholds) with DEGENSAC&quot;&quot;&quot;, &quot;link_to_website&quot;: &quot;https://github.com/kornia/kornia&quot;, &quot;link_to_pdf&quot;: &quot;https://arxiv.org/abs/2007.09699&quot; } . Second part is config_common: it tells the benchmark, which keypoints and descriptors you use. We will also need this names when importing our features during tuning on the validation set. . config_common_dict = {&quot;json_label&quot;: &quot;dog-affnet-hardnet8-degensac&quot;, &quot;keypoint&quot;: &quot;cv2dog&quot;, &quot;descriptor&quot;: &quot;affnethardnet8&quot;, &quot;num_keypoints&quot;: 8000} . Now comes the information how to match your local features. It may vary from dataset to dataset and also be different for the multiview and stereo mode. That is why we will create a template dictionary and change some parameters later. . Specifically, we have to specify, which distance our descriptor prefers: L2, L1 and Hamming are supported. . Then comes the tentative matches filtering. One can pass none for no filtering, snn_ratio_pairwise for Lowe&#39;s SNN ratio and fginn_ratio_pairwise for FGINN. If you are not familiar with filtering strategies, checkout this blogpost: &quot;How to match: to learn or not to learn?&quot;. The threshold is what have to be tunes. . We will use SNN, because of simplicity. Finally, we would like to make sure that tentative matches are cross-consistent, that is why we will enable symmetric matching. . Warning! We will use FLANN approximate nearest neighbor matching for speed-up tuning procedure, but it is better to turn it off for the final submission. . from copy import deepcopy matcher_template_dict = { &quot;method&quot;: &quot;nn&quot;, &quot;distance&quot;: &quot;L2&quot;, &quot;flann&quot;: True, &quot;num_nn&quot;: 1, &quot;filtering&quot;: { &quot;type&quot;: &quot;snn_ratio_pairwise&quot;, &quot;threshold&quot;: 0.90 }, &quot;symmetric&quot;: { &quot;enabled&quot;: True, &quot;reduce&quot;: &quot;both&quot;, } } . Finally, we have to specify robust geometry estimation method. We will pick the default choise from the previous challenge - DEGENSAC. threshold is what have to be tuned, the rest of parameters are already optimal, or fixed by the competition rules -- max_iter. . geom_template_dict = {&quot;method&quot;: &quot;cmp-degensac-f&quot;, &quot;threshold&quot;: 0.5, &quot;confidence&quot;: 0.999999, &quot;max_iter&quot;: 100000, &quot;error_type&quot;: &quot;sampson&quot;, &quot;degeneracy_check&quot;: True, } . Let&#39;s assemble and save our base config. . import json base_config = { &quot;metadata&quot;: metadata_dict, &quot;config_common&quot;: config_common_dict, &quot;config_phototourism_stereo&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_phototourism_multiview&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;colmap&quot;: {}}, &quot;config_pragueparks_stereo&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_pragueparks_multiview&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;colmap&quot;: {}}, &quot;config_googleurban_stereo&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;geom&quot;: deepcopy(geom_template_dict) }, &quot;config_googleurban_multiview&quot;: { &quot;use_custom_matches&quot;: False, &quot;matcher&quot;: deepcopy(matcher_template_dict), &quot;outlier_filter&quot;: { &quot;method&quot;: &quot;none&quot; }, &quot;colmap&quot;: {}} } . Finally, benchmark expects multiple configs, so we have to create a list, and then we can save our config . import json with open(&#39;base_config.json&#39;, &#39;w&#39;) as f: json.dump([base_config], f, indent=2) . Preliminary evaluation . Now let&#39;s check how our features perform on validation set. We have to import our feature to the benchmark and run the benchmark. . I will cheat a little bit here and skip the multiview evaluation. The reason is that it requires colmap, which might be not easy to install. . Importing features . Here we have to provide the same keypoint and descriptor names, as we wrote in json config. The rest of arguments are straightforward: path to features, json, etc. . cd ../image-matching-benchmark/ python -utt import_features.py --kp_name cv2dog --desc_name affnethardnet8 --num_keypoints 8000 --path_features ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8 --path_results ../benchmark-results --subset both --is_challenge false --path_json ../imc2021-sample-kornia-submission/base_config.json --datasets phototourism googleurban pragueparks . Running the evaluation . Now we are ready to run the evaluation . python -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config.json --subset=val --eval_multiview=False --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false . After a while (an 30 min for 32 cores machine), the process will finish and you will see the following log message: . -- Saving to: &quot;packed-val/dog-affnet-hardnet8-degensac.json&quot; . Reading results . Json file with evaluation results is saved to image-matching-benchmark/packed-val/dog-affnet-hardnet8-degensac.json, and some visualizations -- to ../benchmark-visualization/png. . First, we come back to our imc2021-sample-kornia-submission directory: . cd ../imc2021-sample-kornia-submission . Metric, which are used for the competition is mean average accuracy (mAA) at visibility threshold 0.1 . import os hashname=&#39;dog-affnet-hardnet8-degensac&#39; res_fname = os.path.join(&#39;../image-matching-benchmark/packed-val&#39;, f&#39;{hashname}.json&#39;) with open(res_fname, &#39;r&#39;) as f: results = json.load(f) submission_name = results[&#39;config&#39;][&#39;metadata&#39;][&#39;method_name&#39;] datasets = [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;] tasks = [&#39;stereo&#39;] # [&#39;stereo&#39;, &#39;multiview&#39;] #Remember, that we skip colmap evaluations metric = &#39;qt_auc_10_th_0.1&#39; for dset in datasets: for task in tasks: mAA = results[dset][&#39;results&#39;][&#39;allseq&#39;][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] print (f&#39;{submission_name} {task} mAA for {dset} is {mAA:.4f}&#39;) . CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for phototourism is 0.7108 CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for pragueparks is 0.5850 CV-DoG-AffNet-HardNet8 (kornia) stereo mAA for googleurban is 0.3099 . We can also see results sequence-by-sequence . import seaborn as sns sns.set_context(&#39;paper&#39;, font_scale=1.7) seqs = [] mAAs = [] for dset in datasets: for task in tasks: for seq in results[dset][&#39;results&#39;].keys(): if seq == &#39;allseq&#39;: continue mAA = results[dset][&#39;results&#39;][seq][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] mAAs.append(mAA) seqs.append(seq) fig, ax = plt.subplots(figsize=(15,5)) xticks = 2*np.arange(len(seqs)) ax.set_xticks(xticks) ax.bar(xticks, mAAs) ax.set_xticklabels(seqs) ax.set_ylabel(&#39;mAA&#39;) ax.set_xlabel(&#39;Sequence&#39;) . Text(0.5, 0, &#39;Sequence&#39;) . How do our feature correspondences look like? . import cv2 def plot_image_grid(images, ncols=None, cmap=&#39;gray&#39;): #Taken from https://stackoverflow.com/a/66961099/1983544 &#39;&#39;&#39;Plot a grid of images&#39;&#39;&#39; if not ncols: factors = [i for i in range(1, len(images)+1) if len(images) % i == 0] ncols = factors[len(factors) // 2] if len(factors) else len(images) // 4 + 1 nrows = int(len(images) / ncols) + int(len(images) % ncols) imgs = [images[i] if len(images) &gt; i else None for i in range(nrows * ncols)] f, axes = plt.subplots(nrows, ncols, figsize=(3*ncols, 2*nrows)) axes = axes.flatten()[:len(imgs)] for img, ax in zip(imgs, axes.flatten()): if np.any(img): if len(img.shape) &gt; 2 and img.shape[2] == 1: img = img.squeeze() ax.imshow(img, cmap=cmap) imgs = [] VIS_DIR = f&#39;../benchmark-visualization/png/{hashname}&#39; for dset in os.listdir(VIS_DIR): dset_dir = os.path.join(VIS_DIR, dset) for seq in os.listdir(dset_dir): seq_dir = os.path.join(dset_dir, seq, &#39;stereo&#39;) for img_fname in os.listdir(seq_dir): full_fname = os.path.join(seq_dir, img_fname) img = cv2.resize(cv2.cvtColor(cv2.imread(full_fname), cv2.COLOR_BGR2RGB), (200,100)) imgs.append(img) plot_image_grid(imgs) . Hyperparameters tuning . Let&#39;s now tune our hyperparameters, specifically, matching threshold and RANSAC inlier ratio. First we tune RANSAC and then matching ratio. We need to generate jsons for each configuration. . inl_ths = [0.25, 0.5, 0.75, 1.0, 1.5, 2.0, 2.5, 3.0] configs = [] for inl_th in inl_ths: current_config = deepcopy(base_config) for dset in [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;]: current_config[f&#39;config_{dset}_stereo&#39;][&#39;geom&#39;][&#39;threshold&#39;] = inl_th label = current_config[&#39;config_common&#39;][&#39;json_label&#39;] current_config[&#39;config_common&#39;][&#39;json_label&#39;] = f&#39;{label}-inlth-{inl_th}&#39; configs.append(current_config) with open(&#39;ransac_tuning.json&#39;, &#39;w&#39;) as f: json.dump(configs, f, indent=2) . cd ../image-matching-benchmark/ python -utt run.py --run_mode=interactive --json_method=../imc2021-sample-kornia-submission/base_config.json --subset=val --eval_multiview=False --path_data ../imc-2021-data/ --path_results ../benchmark-results --is_challenge false cd ../imc2021-sample-kornia-submission . Now we will write a function, which reads results and picks the best threshold per dataset . hashname=&#39;dog-affnet-hardnet8-degensac&#39; res_dict = {} datasets = [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;] for dset in datasets: res_dict[dset] = {} task = &#39;stereo&#39; metric = &#39;qt_auc_10_th_0.1&#39; for inl_th in inl_ths: res_fname = os.path.join(&#39;../image-matching-benchmark/packed-val&#39;, f&#39;{hashname}-inlth-{inl_th}.json&#39;) try: with open(res_fname, &#39;r&#39;) as f: results = json.load(f) except: continue submission_name = results[&#39;config&#39;][&#39;metadata&#39;][&#39;method_name&#39;] res_dict[inl_th] = {} for dset in datasets: mAA = results[dset][&#39;results&#39;][&#39;allseq&#39;][task][&#39;run_avg&#39;][metric][&#39;mean&#39;] res_dict[dset][inl_th] = mAA fig, ax = plt.subplots(figsize=(5,5)) colors = [&#39;r&#39;,&#39;b&#39;,&#39;k&#39;] final_ths = {} for i, dset in enumerate(datasets): inl_ths = [] mAAs = [] for inl_th, mAA in res_dict[dset].items(): inl_ths.append(inl_th) mAAs.append(mAA) best_th_idx = np.argmax(np.array(mAAs)) best_th = inl_ths[best_th_idx] best_mAA = mAAs[best_th_idx] print (f&#39;Best {dset} mAA = {best_mAA:.4f} with inl_th = {best_th}&#39;) ax.plot(inl_ths, mAAs, label=dset, color=colors[i]) ax.plot(best_th, best_mAA, label = f&#39;{dset}-best&#39;, marker=&#39;x&#39;, linestyle=&#39;&#39;, color=colors[i]) final_ths[dset] = best_th ax.legend() ax.set_ylabel(&#39;mAA&#39;) ax.set_xlabel(&#39;DEGENSAC inlier threshold&#39;) . Best phototourism mAA = 0.7108 with inl_th = 0.5 Best pragueparks mAA = 0.6700 with inl_th = 1.5 Best googleurban mAA = 0.3116 with inl_th = 0.75 . Text(0.5, 0, &#39;DEGENSAC inlier threshold&#39;) . Creating final submission . Its time to create our final submission! . configs = [] current_config = deepcopy(base_config) for dset in [&#39;phototourism&#39;, &#39;pragueparks&#39;, &#39;googleurban&#39;]: current_config[f&#39;config_{dset}_stereo&#39;][&#39;geom&#39;][&#39;threshold&#39;] = final_ths[dset] # I did a little bit of tuning offline for multiview, so we will put it here current_config[f&#39;config_{dset}_multiview&#39;][&#39;matcher&#39;][&#39;filtering&#39;][&#39;threshold&#39;] = 0.95 #Remember, that we should not forget to turn FLANN ofd current_config[f&#39;config_{dset}_multiview&#39;][&#39;matcher&#39;][&#39;flann&#39;] = False current_config[f&#39;config_{dset}_stereo&#39;][&#39;matcher&#39;][&#39;flann&#39;] = False current_config[&#39;metadata&#39;][&#39;method_name&#39;] = &#39;KORNIA TUTORIAL CV-DoG-AffNet-HardNet8&#39; label = current_config[&#39;config_common&#39;][&#39;json_label&#39;] current_config[&#39;config_common&#39;][&#39;json_label&#39;] = f&#39;{label}&#39; configs.append(current_config) print (current_config) with open(&#39;final_submission.json&#39;, &#39;w&#39;) as f: json.dump(configs, f, indent=2) . {&#39;metadata&#39;: {&#39;publish_anonymously&#39;: False, &#39;authors&#39;: &#39;Dmytro Mishkin, Milan Pultar and kornia team&#39;, &#39;contact_email&#39;: &#39;ducha.aiki@gmail.com&#39;, &#39;method_name&#39;: &#39;KORNIA TUTORIAL CV-DoG-AffNet-HardNet8&#39;, &#39;method_description&#39;: &#34;OpeCV SIFT keypoints 8000 features, followed by the AffNet normalization n and HardNet8 descriptor as implemented in kornia. n Matched using the built-in matcher (bidirectional filter with the &#39;both&#39; strategy, n hopefully optimal inlier and ratio test thresholds) with DEGENSAC&#34;, &#39;link_to_website&#39;: &#39;https://github.com/kornia/kornia&#39;, &#39;link_to_pdf&#39;: &#39;https://arxiv.org/abs/2007.09699&#39;}, &#39;config_common&#39;: {&#39;json_label&#39;: &#39;dog-affnet-hardnet8-degensac&#39;, &#39;keypoint&#39;: &#39;cv2dog&#39;, &#39;descriptor&#39;: &#39;affnethardnet8&#39;, &#39;num_keypoints&#39;: 8000}, &#39;config_phototourism_stereo&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.9}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;geom&#39;: {&#39;method&#39;: &#39;cmp-degensac-f&#39;, &#39;threshold&#39;: 0.5, &#39;confidence&#39;: 0.999999, &#39;max_iter&#39;: 100000, &#39;error_type&#39;: &#39;sampson&#39;, &#39;degeneracy_check&#39;: True}}, &#39;config_phototourism_multiview&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.95}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;colmap&#39;: {}}, &#39;config_pragueparks_stereo&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.9}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;geom&#39;: {&#39;method&#39;: &#39;cmp-degensac-f&#39;, &#39;threshold&#39;: 1.5, &#39;confidence&#39;: 0.999999, &#39;max_iter&#39;: 100000, &#39;error_type&#39;: &#39;sampson&#39;, &#39;degeneracy_check&#39;: True}}, &#39;config_pragueparks_multiview&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.95}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;colmap&#39;: {}}, &#39;config_googleurban_stereo&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.9}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;geom&#39;: {&#39;method&#39;: &#39;cmp-degensac-f&#39;, &#39;threshold&#39;: 0.75, &#39;confidence&#39;: 0.999999, &#39;max_iter&#39;: 100000, &#39;error_type&#39;: &#39;sampson&#39;, &#39;degeneracy_check&#39;: True}}, &#39;config_googleurban_multiview&#39;: {&#39;use_custom_matches&#39;: False, &#39;matcher&#39;: {&#39;method&#39;: &#39;nn&#39;, &#39;distance&#39;: &#39;L2&#39;, &#39;flann&#39;: False, &#39;num_nn&#39;: 1, &#39;filtering&#39;: {&#39;type&#39;: &#39;snn_ratio_pairwise&#39;, &#39;threshold&#39;: 0.95}, &#39;symmetric&#39;: {&#39;enabled&#39;: True, &#39;reduce&#39;: &#39;both&#39;}}, &#39;outlier_filter&#39;: {&#39;method&#39;: &#39;none&#39;}, &#39;colmap&#39;: {}}} . Submission Zip file should have folder structure as follow: â”œâ”€â”€ config.json â”œâ”€â”€ [Dataset 1] â”‚ â”œâ”€â”€ [Sequence 1] â”‚ â”‚ â”œâ”€â”€ keypoints.h5 â”‚ â”‚ â”œâ”€â”€ descriptors.h5 â”‚ â”‚ â”œâ”€â”€ matches.h5 â”‚ â”œâ”€â”€ [Sequence 2] â”‚ â”‚ â”œâ”€â”€ ... â”œâ”€â”€ [Dataset 2] â”‚ â”œâ”€â”€ ... . So we have to just copy our features, add config and zip them. . cp final_submission.json extracted/cv2-dog-affnet-hardnet8/config.json cd extracted/cv2-dog-affnet-hardnet8 zip -r submission.zip * . Last step before the submission - check the submission for correctness with provided script . cd ../../../image-matching-benchmark python -utt submission_validator.py --submit_file_path ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission.zip --benchmark_repo_path . --raw_data_path ../imc-2021-data/ --datasets googleurban phototourism pragueparks . If everything is correct, you will see: . Validating method 1/1: &quot;dog-affnet-hardnet8-degensac&quot; [&#39;googleurban&#39;, &#39;phototourism&#39;, &#39;pragueparks&#39;] Running: googleurban, stereo track Running: googleurban, multiview track Running: phototourism, stereo track Running: phototourism, multiview track Running: pragueparks, stereo track Running: pragueparks, multiview track Validating key &quot;config_googleurban_stereo&quot; Validating key &quot;config_googleurban_multiview&quot; Validating key &quot;config_phototourism_stereo&quot; Validating key &quot;config_phototourism_multiview&quot; Validating key &quot;config_pragueparks_stereo&quot; Validating key &quot;config_pragueparks_multiview&quot; . And file submission_log.txt will appear near our .zip file. . cat ../imc2021-sample-kornia-submission/extracted/cv2-dog-affnet-hardnet8/submission_log.txt . Submission is in proper format, please submit to IMW 2021 website. . That&#39;s all, folks! We can submit! But, please, do not just submit this sample submission - make your own :) . P.S. This post is originally appeared at Wide Baseline Stereo blog. .",
            "url": "https://librecv.github.io/blog/2021/05/18/submitting-to-IMC2021-step-by-step.html",
            "relUrl": "/2021/05/18/submitting-to-IMC2021-step-by-step.html",
            "date": " â€¢ May 18, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Continuum - Simple Management of Complex Continual Learning Scenarios",
            "content": "Introduction . Continual Learning is a field of machine learning where the data distribution changes through time. For instance, instead of learning to classify all animals in the world at once, the model learns to classify them 10 by 10. To experiment in this setting, the data management is quite different from usual machine learning. . Continuum is a Python based library that proposes various data loaders for Continual Learning scenarios. It is designed to prevent researchers from spending time on designing classical data loaders and eliminate time-consuming errors. With Continuum, it is possible to focus on the model design using multiple scenarios directly. It also includes tools fro logging algorithm results and compute continual learning metrics. . Our philosophy is inspired by UNIX&#39;s: we aim to do few things but do it well without too many assumptions about your research code. Continuum is designed to be pluggable seamlessly in any codebase. . Github repository | Website documentation | . Installation . The library can be installed easily thanks to pip. . %%capture !pip install continuum . Create your First Scenario . One of the simplest scenarios that continuum proposes is the split MNIST. . This scenario consists of learning MNIST by part. We split MNIST into several parts that contain different classes, i.e. we learn incrementally new classes. In our case, we see at first only images of &quot;0&quot; and &quot;1&quot;, then images of &quot;2&quot; and &quot;3&quot;, etc. Continuum proposes to use the class ClassIncremental to split the MNIST dataset into tasks. . First, we create an dataset object that contains the MNIST dataset. . %%capture from six.moves import urllib opener = urllib.request.build_opener() opener.addheaders = [(&#39;User-agent&#39;, &#39;Mozilla/5.0&#39;)] urllib.request.install_opener(opener) from continuum.datasets import MNIST # Import the dataset dataset = MNIST(&quot;.&quot;, download=True, train=True) . Then, we create a scenario of 5 tasks containing each the data of two classes. . . from continuum import ClassIncremental # Import the scenarios template # Create the scenario object. scenario = ClassIncremental( dataset, increment=2 # each new task contains, 2 classes ) print(&quot;******************************************&quot;) print(f&quot;Number of classes: {scenario.nb_classes}.&quot;) print(f&quot;Number of tasks: {scenario.nb_tasks}.&quot;) print(&quot;******************************************&quot;) . ****************************************** Number of classes: 10. Number of tasks: 5. ****************************************** . The scenario object can be iterate into the 5 TaskSets that can be used with a Pytorch dataloader. . Each batch contains three kinds of tensors: . x: the pixels of the images | y: the labels of the images (i.e. 0, 1, 2, 3, 4, etc.) | t: the task identifiers of the images. | . . from torch.utils.data import DataLoader import matplotlib.pyplot as plt fig, axs = plt.subplots(1, len(scenario), figsize=(15,3)) for task_id, taskset in enumerate(scenario): # plot samples for each tasks ind_plot = 100 + len(scenario)*10 + task_id+1 plt.subplot(ind_plot) taskset.plot(nb_samples=25) plt.title(f&quot;Samples n task {task_id}&quot;) # convert taskset into a the data loader for one task loader = DataLoader(taskset, batch_size=10) for x, y, t in loader: # x: data, y: label, t:task label # Train your model here print(&quot;**********************&quot;) print(f&quot;Task : {task_id}&quot;) print(f&quot;Data Shape : {x.shape}&quot;) print(f&quot;Labels : {y}&quot;) print(f&quot;Task label : {t}&quot;) break # we do not want to iterate through all the data in this example . ********************** Task : 0 Data Shape : torch.Size([10, 1, 28, 28]) Labels : tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0]) Task label : tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) ********************** Task : 1 Data Shape : torch.Size([10, 1, 28, 28]) Labels : tensor([2, 3, 3, 3, 2, 2, 3, 2, 3, 3]) Task label : tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) ********************** Task : 2 Data Shape : torch.Size([10, 1, 28, 28]) Labels : tensor([5, 4, 4, 5, 4, 4, 5, 5, 4, 4]) Task label : tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) ********************** Task : 3 Data Shape : torch.Size([10, 1, 28, 28]) Labels : tensor([6, 7, 6, 7, 6, 6, 7, 6, 7, 7]) Task label : tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3]) ********************** Task : 4 Data Shape : torch.Size([10, 1, 28, 28]) Labels : tensor([9, 8, 9, 9, 8, 9, 8, 9, 9, 8]) Task label : tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4]) . Note that you can index the scenario object to select a specific taskset or set of tasksets: . task_set = scenario[1] # Second task task_set = scenario[:3] # First three tasks task_set = scenario[::2] # All even tasks task_set = scenario[:] # All tasks . The ClassIncremental scenario, as most proposed scenarios, can accept different datasets than MNIST. Here is a list of the existing datasets: . Name Nb classes Image Size Automatic Download Type . MNIST | 10 | 28x28x1 | YES | Images | . Fashion MNIST | 10 | 28x28x1 | YES | Images | . KMNIST | 10 | 28x28x1 | YES. | Images | . EMNIST | 10 | 28x28x1 | YES | Images | . QMNIST | 10 | 28x28x1 | YES | Images | . MNIST Fellowship | 30 | 28x28x1 | YES | Images | . CIFAR10 | 10 | 32x32x3 | YES | Images | . CIFAR100 | 100 | 32x32x3 | YES | Images | . CIFAR Fellowship | 110 | 32x32x3 | YES | Images | . ImageNet100 | 100 | 224x224x3 | NO | Images | . ImageNet1000 | 1000 | 224x224x3 | NO | Images | . CORe50 | 50 | 224x224x3 | YES | Images | . CORe50-v2-79 | 50 | 224x224x3 | YES | Images | . CORe50-v2-196 | 50 | 224x224x3 | YES | Images | . CORe50-v2-391 | 50 | 224x224x3 | YES | Images | . Synbols | 50 | 32x32x3 | YES | Images | . MultiNLI | 5 | | YES | Text | . Stream51 | 51 | ?x?x3 | YES | Images | . PascalVOC-2012 | 20 + 1 | ?x?x3 | YES | Segmentation | . This list may not be up-to-date, as new datasets are released regularly. Furthermore, you can easily add your own dataset with only a few lines of code and benefit from the Continuum ecosystem: Adding Your Own Datasets ). . The most recent addition (at the time of writing) to Continuuum is PascalVOC-2012: a segmentation dataset where images are partially labelized, a challenging setting: . . ClassIncremental is not the only scenario we propose. For example, the InstanceIncremental scenario consists of learning several tasks with the same classes, but the task data are always different. . Famous examples of instance incremental scenarios are permuted MNIST and rotated MNIST. . In permuted MNIST, each task contains all MNIST data with a fixed permutation of pixels, but each task has a different permutation. In the following code snippet, we create a 5 tasks permuted MNIST scenario. The scenario created with class Permutation can be used like previously with the ClassIncremental scenario. . NB: The Permutation scenario is a scpecial case of InstanceIncremental scenario. . from continuum import Permutations from continuum.datasets import MNIST train_dataset = MNIST(&quot;.&quot;, download=True, train=True) train_scenario = Permutations(cl_dataset=dataset, nb_tasks=5) # create test scenario with all test data in a single tasks test_dataset = MNIST(&quot;.&quot;, download=True, train=True) test_scenario = Permutations(cl_dataset=dataset, nb_tasks=5) . Logging with Continuum . Continuum also proposes tools to log algorithms&#39; performance and compute easily various continual learning metrics. . import torch from continuum.metrics import Logger from torch.utils.data import DataLoader logger = Logger() # training for taskset in train_scenario: loader = DataLoader(taskset, batch_size=64) for (x,y,t) in loader: #predictions = model(x,y,t) # if we had a model, we could get a prediction # we will assume here the model is perfect, therefore, # we replace the prediction by the expected labels preds = y logger.add([preds, y, t]) print(f&quot;Online accuracy on train: {logger.online_accuracy*100} %&quot;) # evaluation for taskset in test_scenario: loader = DataLoader(taskset, batch_size=64) for (x,y,t) in loader: #once again we have perfect prediction preds = y logger.add([preds, y, t], subset=&#39;test&#39;) print(f&quot;Test Accuracy: {logger.accuracy*100} %&quot;) . Online accuracy on train: 100.0 % Online accuracy on train: 100.0 % Online accuracy on train: 100.0 % Online accuracy on train: 100.0 % Online accuracy on train: 100.0 % Test Accuracy: 100.0 % Test Accuracy: 100.0 % Test Accuracy: 100.0 % Test Accuracy: 100.0 % . The continuum logger can be used to calculate several other continual learning metrics: . Name Code â†‘ / â†“ . Accuracy | accuracy | â†‘ . | . Accuracy A | accuracy_A | â†‘ . | . Backward Transfer | backward_transfer | â†‘ . | . Positive Backward Transfer | positive_backward_transfer | â†‘ . | . Remembering | remembering | â†‘ . | . Forward Transfer | forward_transfer | â†‘ . | . Forgetting | forgetting | â†“ . | . Model Size Growth | model_size_growth | â†“ . | . Online Accuracy | online_accuracy | â†‘ . | . (Details here) . Conclusion . Continuum is an open-source project which aims at simplifying data management for continual learning algorithms. It aims at covering the various types of scenarios in the continual learning research field, and it is developed such as being easily adaptable to specific needs. . Continuum is made to save development time, reduce code size in continual project, and avoid classical data loader bugs. . Moreover, we believe that using a common plateform for continual learning experiments will benefit the reproducibility problems that researchers often experiment with. We believe by providing the right tools, Continuum can enable better research in Continual Learning: several research papers already used this framework, and the Continual Learning workshop of CVPR 2021 will hold a competition where the participants will also have to use Continuum. .",
            "url": "https://librecv.github.io/blog/continual%20learning/pytorch/2021/03/19/Continual-Scenarios-With-Continuum.html",
            "relUrl": "/continual%20learning/pytorch/2021/03/19/Continual-Scenarios-With-Continuum.html",
            "date": " â€¢ Mar 19, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Pix2Pix - Image to image translation with Conditional Adversarial Networks",
            "content": "Author Introduction . Hi! My name is Aniket Maurya. I am a Machine Learning Engineer at Quinbay Technologies, India. I research and build ML products. I like to share my limited knowledge of Machine Learning and Deep Learning with on my blog or YouTube channel.You can connect with me on Linkedin/Twitter. . Introduction to Conditional Adversarial Networks . Image to Image translation means transforming the given source image into a different image. Gray scale image to colour image conversion is one such example of image of image translation. . In this tutorial we will discuss GANs, a few points from Pix2Pix paper and implement the Pix2Pix network to translate segmented facade into real pictures. We will create the Pix2Pix model in PyTorch and use PyTorch lightning to avoid boilerplates. . Authors of Image-to-Image Translation with Conditional Adversarial Networks paper has also made the source code publically available on GitHub.&gt; A more detailed tutorial on GANs can be found here - Yann LeCunâ€™s Deep Learning Course at CDS . GANs are Generative models that learns a mapping from random noise vector (z) to an output image. G(z) -&gt; Image (y) . For example, GANs can learn mapping from random normal vectors to generate smiley images. For training such a GAN we just need a set of smiley images and train the GAN with an adversarial loss ðŸ™‚. After the model is trained we can use random normal noise vectors to generate images that were not in the training dataset. . But what if we want to build a network in such a way that we can control what the model will generate. In our case we want the model to generate a laughing smiley. . Conditional GANs are Generative networks which learn mapping from random noise vectors and a conditional vector to output an image. Suppose we have 4 types of smileys - smile, laugh, sad and angry (ðŸ™‚ ðŸ˜‚ ðŸ˜” ðŸ˜¡). So our class vector for smile ðŸ™‚ can be (1,0,0,0), laugh can be ðŸ˜‚ (0,1,0,0) and similarly for others. Here the conditional vector is the smiley embedding. . During training of the generator the conditional image is passed to the generator and fake image is generated. The fake image is then passed through the discriminator along with the conditional image, both fake image and conditional image are concatenated. Discriminator penalizes the generator if it correctly classifies the fake image as fake. . Pix2Pix . Pix2Pix is an image-to-image translation Generative Adversarial Networks that learns a mapping from an image X and a random noise Z to output image Y or in simple language it learns to translate the source image into a different distribution of image. . During the time Pix2Pix was released, several other works were also using Conditional GANs on discrete labels. Pix2Pix uses a U-Net based architecture for the Generator and for the Discriminator a PatchGAN Classifier is used. . . Here is what Phillipi has to say about PatchGAN - . Pix2Pix Generator is an U-Net based architecture which is an encoder-decoder network with skip connections. The name U-Net highlights the structure of the &quot;U&quot; shaped network. Both generator and discriminator uses Convolution-BatchNorm-ReLu like module or in simple words we can say that it is the unit block of the generator and discriminator. Skip connections are added between each layer i and layer nâˆ’i, where n is the total number of layers. At each skip connection all the channels from current layer i are concatenated with all the channels at n-i layer. . Lets understand it more with code. . import os from glob import glob from pathlib import Path import matplotlib.pyplot as plt import pytorch_lightning as pl import torch from PIL import Image from torch import nn from torch.utils.data import DataLoader, Dataset from torchvision import transforms from torchvision.transforms.functional import center_crop from torchvision.utils import make_grid from tqdm.auto import tqdm . . Uncomment the below code to download the dataset . !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz !tar -xvf facades.tar.gz . After downloading the dataset we create Dataloader which loads our conditional and real image. . path = &quot;./facades/train/&quot; class FacadesDataset(Dataset): def __init__(self, path, target_size=None): self.filenames = glob(str(Path(path) / &quot;*&quot;)) self.target_size = target_size def __len__(self): return len(self.filenames) def __getitem__(self, idx): filename = self.filenames[idx] image = Image.open(filename) image = transforms.functional.to_tensor(image) image_width = image.shape[2] real = image[:, :, : image_width // 2] condition = image[:, :, image_width // 2 :] target_size = self.target_size if target_size: condition = nn.functional.interpolate(condition, size=target_size) real = nn.functional.interpolate(real, size=target_size) return real, condition . In the first part of U-Net shaped network the layer size decreases, we create a DownSampleConv module for this. This module will contain the unit block that we just created ConvBlock. . class DownSampleConv(nn.Module): def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True): &quot;&quot;&quot; Paper details: - C64-C128-C256-C512-C512-C512-C512-C512 - All convolutions are 4Ã—4 spatial filters applied with stride 2 - Convolutions in the encoder downsample by a factor of 2 &quot;&quot;&quot; super().__init__() self.activation = activation self.batchnorm = batchnorm self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding) if batchnorm: self.bn = nn.BatchNorm2d(out_channels) if activation: self.act = nn.LeakyReLU(0.2) def forward(self, x): x = self.conv(x) if self.batchnorm: x = self.bn(x) if self.activation: x = self.act(x) return x . Now in the second part the network expands and so we create UpSampleConv . class UpSampleConv(nn.Module): def __init__( self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True, dropout=False ): super().__init__() self.activation = activation self.batchnorm = batchnorm self.dropout = dropout self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding) if batchnorm: self.bn = nn.BatchNorm2d(out_channels) if activation: self.act = nn.ReLU(True) if dropout: self.drop = nn.Dropout2d(0.5) def forward(self, x): x = self.deconv(x) if self.batchnorm: x = self.bn(x) if self.dropout: x = self.drop(x) return x . Now the basic blocks of the Pix2Pix generated is created, we create the generator module. Generator is formed of expanding and contracting layers. The first part network contracts and then expands again, i.e. first we have encoder block and then decoder block. Below is the encoder-decoder of U-Net network configuration from official paper. Here C denotes the unit block that we created ConvBlock and D denotes Drop Out with value 0.5. In the decoder, the output tensors from n-i layer of encoder concatenates with i layer of the decoder. Also the first three blocks of the decoder has drop out layers. . Encoder: C64-C128-C256-C512-C512-C512-C512-C512 Decoder: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128 . class Generator(nn.Module): def __init__(self, in_channels, out_channels): &quot;&quot;&quot; Paper details: - Encoder: C64-C128-C256-C512-C512-C512-C512-C512 - All convolutions are 4Ã—4 spatial filters applied with stride 2 - Convolutions in the encoder downsample by a factor of 2 - Decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128 &quot;&quot;&quot; super().__init__() # encoder/donwsample convs self.encoders = [ DownSampleConv(in_channels, 64, batchnorm=False), # bs x 64 x 128 x 128 DownSampleConv(64, 128), # bs x 128 x 64 x 64 DownSampleConv(128, 256), # bs x 256 x 32 x 32 DownSampleConv(256, 512), # bs x 512 x 16 x 16 DownSampleConv(512, 512), # bs x 512 x 8 x 8 DownSampleConv(512, 512), # bs x 512 x 4 x 4 DownSampleConv(512, 512), # bs x 512 x 2 x 2 DownSampleConv(512, 512, batchnorm=False), # bs x 512 x 1 x 1 ] # decoder/upsample convs self.decoders = [ UpSampleConv(512, 512, dropout=True), # bs x 512 x 2 x 2 UpSampleConv(1024, 512, dropout=True), # bs x 512 x 4 x 4 UpSampleConv(1024, 512, dropout=True), # bs x 512 x 8 x 8 UpSampleConv(1024, 512), # bs x 512 x 16 x 16 UpSampleConv(1024, 256), # bs x 256 x 32 x 32 UpSampleConv(512, 128), # bs x 128 x 64 x 64 UpSampleConv(256, 64), # bs x 64 x 128 x 128 ] self.decoder_channels = [512, 512, 512, 512, 256, 128, 64] self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1) self.tanh = nn.Tanh() self.encoders = nn.ModuleList(self.encoders) self.decoders = nn.ModuleList(self.decoders) def forward(self, x): skips_cons = [] for encoder in self.encoders: x = encoder(x) skips_cons.append(x) skips_cons = list(reversed(skips_cons[:-1])) decoders = self.decoders[:-1] for decoder, skip in zip(decoders, skips_cons): x = decoder(x) # print(x.shape, skip.shape) x = torch.cat((x, skip), axis=1) x = self.decoders[-1](x) # print(x.shape) x = self.final_conv(x) return self.tanh(x) . Discriminator . A discriminator is a ConvNet which learns to classify images into discrete labels. In GANs, discriminators learns to predict whether the given image is real or fake. PatchGAN is the discriminator used for Pix2Pix. Its architecture is different from a typical image classification ConvNet because of the output layer size. In convnets output layer size is equal to the number of classes while in PatchGAN output layer size is a 2D matrix. . Now we create our Discriminator - PatchGAN. In this network we use the same DownSampleConv module that we created for generator. . class PatchGAN(nn.Module): def __init__(self, input_channels): super().__init__() self.d1 = DownSampleConv(input_channels, 64, batchnorm=False) self.d2 = DownSampleConv(64, 128) self.d3 = DownSampleConv(128, 256) self.d4 = DownSampleConv(256, 512) self.final = nn.Conv2d(512, 1, kernel_size=1) def forward(self, x, y): x = torch.cat([x, y], axis=1) x0 = self.d1(x) x1 = self.d2(x0) x2 = self.d3(x1) x3 = self.d4(x2) xn = self.final(x3) return xn . Loss Function . Loss function used in Pix2Pix are Adversarial loss and Reconstruction loss. Adversarial loss is used to penalize the generator to predict more realistic images. In conditional GANs, generators job is not only to produce realistic image but also to be near the ground truth output. Reconstruction Loss helps network to produce the realistic image near the conditional image. . adversarial_loss = nn.BCEWithLogitsLoss() reconstruction_loss = nn.L1Loss() . # https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch def _weights_init(m): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): torch.nn.init.normal_(m.weight, 0.0, 0.02) if isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) torch.nn.init.constant_(m.bias, 0) def display_progress(cond, fake, real, figsize=(10,5)): cond = cond.detach().cpu().permute(1, 2, 0) fake = fake.detach().cpu().permute(1, 2, 0) real = real.detach().cpu().permute(1, 2, 0) fig, ax = plt.subplots(1, 3, figsize=figsize) ax[0].imshow(cond) ax[2].imshow(fake) ax[1].imshow(real) plt.show() . . class Pix2Pix(pl.LightningModule): def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, display_step=25): super().__init__() self.save_hyperparameters() self.display_step = display_step self.gen = Generator(in_channels, out_channels) self.patch_gan = PatchGAN(in_channels + out_channels) # intializing weights self.gen = self.gen.apply(_weights_init) self.patch_gan = self.patch_gan.apply(_weights_init) self.adversarial_criterion = nn.BCEWithLogitsLoss() self.recon_criterion = nn.L1Loss() def _gen_step(self, real_images, conditioned_images): # Pix2Pix has adversarial and a reconstruction loss # First calculate the adversarial loss fake_images = self.gen(conditioned_images) disc_logits = self.patch_gan(fake_images, conditioned_images) adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits)) # calculate reconstruction loss recon_loss = self.recon_criterion(fake_images, real_images) lambda_recon = self.hparams.lambda_recon return adversarial_loss + lambda_recon * recon_loss def _disc_step(self, real_images, conditioned_images): fake_images = self.gen(conditioned_images).detach() fake_logits = self.patch_gan(fake_images, conditioned_images) real_logits = self.patch_gan(real_images, conditioned_images) fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits)) real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits)) return (real_loss + fake_loss) / 2 def configure_optimizers(self): lr = self.hparams.learning_rate gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr) disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr) return disc_opt, gen_opt def training_step(self, batch, batch_idx, optimizer_idx): real, condition = batch loss = None if optimizer_idx == 0: loss = self._disc_step(real, condition) self.log(&#39;PatchGAN Loss&#39;, loss) elif optimizer_idx == 1: loss = self._gen_step(real, condition) self.log(&#39;Generator Loss&#39;, loss) if self.current_epoch%self.display_step==0 and batch_idx==0 and optimizer_idx==1: fake = self.gen(condition).detach() display_progress(condition[0], fake[0], real[0]) return loss . Now that the network is implemented now we are ready to train. You can also modify the dataloader and train on custom dataset. . Recently I contributed to Pytorch Lightning and now the Pix2Pix model is also available on Pytorch Lightning Bolts, feel free to try that out as well. Hope you liked the article! Happy training ðŸ˜ƒ . dataset = FacadesDataset(path, target_size=target_size) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) pix2pix = Pix2Pix(3, 3, learning_rate=lr, lambda_recon=lambda_recon, display_step=display_step) trainer = pl.Trainer(max_epochs=1000, gpus=1) trainer.fit(pix2pix, dataloader) . GPU available: True, used: True TPU available: None, using: 0 TPU cores | Name | Type | Params 0 | gen | Generator | 54.4 M 1 | patch_gan | PatchGAN | 2.8 M 2 | adversarial_criterion | BCEWithLogitsLoss | 0 3 | recon_criterion | L1Loss | 0 57.2 M Trainable params 0 Non-trainable params 57.2 M Total params 228.713 Total estimated model params size (MB) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). .",
            "url": "https://librecv.github.io/blog/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html",
            "relUrl": "/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html",
            "date": " â€¢ Feb 13, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Deep Learning Experiments with Task Spooler",
            "content": "Introduction . Task Spooler was originally developed by Lluis Batlle i Rossell but is no longer maintained. The branch introduced here is a fork of the original program with more features including GPU support. . Installation . First, you can clone Task Spooler from Github. Optionally, you can choose a different version by checking out another tag. In this tutorial, I will use the latest version on master. . %%capture !git clone https://github.com/justanhduc/task-spooler . Next, you need to create a CUDA_HOME environment variable to point to the CUDA root directory. Then, you can execute the given install script. . !cd task-spooler/ &amp;&amp; CUDA_HOME=/usr/local/cuda ./reinstall . rm -f *.o ts cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c main.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c server.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c server_start.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c client.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c msgdump.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c jobs.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c execute.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c msg.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c mail.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c error.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c signals.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c list.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c print.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c info.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c env.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c tail.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -L/usr/local/cuda/lib64 -I/usr/local/cuda/include -lpthread -c gpu.c In file included from gpu.c:6:0: /usr/local/cuda/include/nvml.h:6208:51: warning: ISO C restricts enumerator values to range of â€˜intâ€™ [-Wpedantic] NVML_VGPU_COMPATIBILITY_LIMIT_OTHER = 0x80000000, //!&lt; Compatibility is limited by an undefined factor. ^~~~~~~~~~ cc -o ts main.o server.o server_start.o client.o msgdump.o jobs.o execute.o msg.o mail.o error.o signals.o list.o print.o info.o env.o tail.o gpu.o -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -I/usr/local/cuda/include -lpthread -lcudart -lcublas -fopenmp -lnvidia-ml make: &#39;uninstall&#39; is up to date. install -c -d /usr/local/bin install -c ts /usr/local/bin install -c -d /usr/local/share/man/man1 install -c -m 644 ts.1 /usr/local/share/man/man1 . Basics of Task Spooler . First look . !ts . ID State Output E-Level Time GPUs Command [run=0/1] . The interface of Task Spooler can be seen like above by simply executing ts without argument. In the figure above, ID refers to job ID. There are four main types of State: running indicates that a job is currently running, queued that a CPU job is waiting to be executed, allocating is a queued GPU job, and running means the job is currently being executed. When a job is executed, the stdout stream is redirected to a file under the Output tab. These log files will never automatically deleted even after the job list is cleared. E-Level captures and displays the return error of a process. Time indicates the running time of a job. The running command is shown in the Command column. The numbers inside the square bracket next to Command specify the number of currently running jobs and the maximum jobs (slots) that can be run in parallel. For example, in the figure above, there is no running job and you can run at most one job in parallel, respectively. The maximum slot number can be adjusted manually. . Queuing your first job . Jobs can be added by simply appending ts in front of your command. For e.g., to run make the system sleep for 10 seconds using Task Spooler, execute . !ts sleep 10 !ts !sleep 10 # lets check ts again after 10 seconds !ts . 0 ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.j0MGwO 0 sleep 10 ID State Output E-Level Time GPUs Command [run=0/1] 0 finished /tmp/ts-out.j0MGwO 0 10.00s 0 sleep 10 . You can see that the first job with ID 0 is currently running, and the other job is being queued. After 10 seconds, the first job will finish with an E-Level of 0 and the second job will start. . To enable running more jobs in parallel, you can increase the maximum slot number by using a -S flag followed by the desired number. For instance, . !ts -S 4 !ts . ID State Output E-Level Time GPUs Command [run=0/4] 0 finished /tmp/ts-out.j0MGwO 0 10.00s 0 sleep 10 . The command above allows you to run 4 jobs at the same time. You can verify by typing ts and the last number in the square bracket should change to 4. Let&#39;s try queuing 5 jobs at once and this time we should increase the sleep time to 100 so that the job doesn&#39;t end too fast. You should be able to see something like this . !ts sleep 100 !ts sleep 20 !ts sleep 30 !ts sleep 40 !ts sleep 10 !ts . 1 2 3 4 5 ID State Output E-Level Time GPUs Command [run=4/4] 1 running /tmp/ts-out.xDq00e 0 sleep 100 2 running /tmp/ts-out.HUzUai 0 sleep 20 3 running /tmp/ts-out.sYcGno 0 sleep 30 4 running /tmp/ts-out.ArV4nv 0 sleep 40 5 queued (file) 0 sleep 10 0 finished /tmp/ts-out.j0MGwO 0 10.00s 0 sleep 10 . Viewing command outputs . As mentioned above, the stdout of the command is redirected to a file specified in the Output column. To manually see the written output, you can simply look for that file. But of course Task Spooler is more than that. It lets you read the outputs contents in two ways via the flags -t and -c. . -c, which stands for cat, allows you to see all the output from the beginning to the end. -t, which means tail, displays only the last 10 lines of the output. Let&#39;s try them out. First, we can something that can produce a lot of texts, like ls, df or du. The choice is yours. For me, I ran ts ls /usr/bin. The job ID of the command in my case is 0 so to visualize the whole output, I used ts -c 0. It displayed a long list of excutable files. When I typed ts -t 0, it showed only the last 10 lines. . !ts -K # reset Task Spooler. it will be introduced later !ts ls /usr/bin !ts -t 0 . 0 yes zdump zip zipcloak zipdetails zipgrep zipinfo zipnote zipsplit zrun . . %%capture !ts -c 0 . Miscs . There are many other flag options to manage your tasks. First of all, to see all the available options, use a -h options. Among these, the ones you probably will use most are -r, -C, -k, -T and -K. To remove a queued or finished job (with finished, queued or allocating status), use -r with optionally a job ID. For example, ts -r removes the last added job if it is not running yet. ts -r 10 removes the job with ID 10. If the job is successfully removed, it should disappear from the job list. . !ts -K !ts -S 2 # lets run 2 tasks at a time !ts sleep 100 !ts sleep 100 !ts sleep 100 !ts . 0 1 2 ID State Output E-Level Time GPUs Command [run=2/2] 0 running /tmp/ts-out.gClvpl 0 sleep 100 1 running /tmp/ts-out.rW9nIv 0 sleep 100 2 queued (file) 0 sleep 100 . !ts -r 2 # remove job 2 !ts . ID State Output E-Level Time GPUs Command [run=2/2] 0 running /tmp/ts-out.gClvpl 0 sleep 100 1 running /tmp/ts-out.rW9nIv 0 sleep 100 . To kill a running job, use ts -k &lt;jobid&gt;. . !ts -k 0 # lets kill job 0 !ts . ID State Output E-Level Time GPUs Command [run=1/2] 1 running /tmp/ts-out.rW9nIv 0 sleep 100 0 finished /tmp/ts-out.gClvpl -1 8.07s 0 sleep 100 . !ts -S 5 !ts sleep 100 !ts sleep 100 !ts sleep 100 !ts . 3 4 5 ID State Output E-Level Time GPUs Command [run=4/5] 1 running /tmp/ts-out.rW9nIv 0 sleep 100 3 running /tmp/ts-out.BeUKip 0 sleep 100 4 running /tmp/ts-out.uFu50z 0 sleep 100 5 running /tmp/ts-out.o0hd1F 0 sleep 100 0 finished /tmp/ts-out.gClvpl -1 8.07s 0 sleep 100 . To kill all running jobs, use ts -T. . !ts -T # terminates all running jobs !ts . ID State Output E-Level Time GPUs Command [run=0/5] 0 finished /tmp/ts-out.gClvpl -1 8.07s 0 sleep 100 1 finished /tmp/ts-out.rW9nIv -1 22.42s 0 sleep 100 5 finished /tmp/ts-out.o0hd1F -1 8.84s 0 sleep 100 3 finished /tmp/ts-out.BeUKip -1 9.06s 0 sleep 100 4 finished /tmp/ts-out.uFu50z -1 8.95s 0 sleep 100 . To clear all the finished jobs from the list, use -C without argument. . !ts sleep 100 !ts -C # clear job list !ts . 6 ID State Output E-Level Time GPUs Command [run=1/5] 6 running /tmp/ts-out.bOY0Sx 0 sleep 100 . Finally, ts -K will kill the Task Spooler process. . !ts -K # lets kill Task Spooler !ts # then restarts . ID State Output E-Level Time GPUs Command [run=0/1] . There are some useful flags when scheduling tasks as well. You may want to execute a task only after a certain job finishes. In this case you can use the flag -d with no argument to make your future task depend on the last added job, -D with a comma separated list of job IDs which are the IDs of the jobs that the to-be-run task depends on, and -W followed by a list of IDs, which states that the dependent job will run iff all the dependencies finish with exit code 0. For example, . !ts -S 10 # lets queue 3 jobs first !ts sleep 100 !ts sleep 100 !ts sleep 200 !ts . 0 1 2 ID State Output E-Level Time GPUs Command [run=3/10] 0 running /tmp/ts-out.1wh18P 0 sleep 100 1 running /tmp/ts-out.aqr1P0 0 sleep 100 2 running /tmp/ts-out.SLCGX7 0 sleep 200 . !ts -d sleep 10 # does not care about exit code !ts -D 0,1,3 sleep 10 # runs after jobs 0, 1 and 3 !ts -W 0,2,3 sleep 10 # to run this job, jobs 0, 2 and 3 need to finish well !ts . 3 4 5 ID State Output E-Level Time GPUs Command [run=3/10] 0 running /tmp/ts-out.1wh18P 0 sleep 100 1 running /tmp/ts-out.aqr1P0 0 sleep 100 2 running /tmp/ts-out.SLCGX7 0 sleep 200 3 queued (file) 0 [2]&amp;&amp; sleep 10 4 queued (file) 0 [0,1,3]&amp;&amp; sleep 10 5 queued (file) 0 [0,2,3]&amp;&amp; sleep 10 . !ts -k 2 !ts . ID State Output E-Level Time GPUs Command [run=3/10] 0 running /tmp/ts-out.1wh18P 0 sleep 100 1 running /tmp/ts-out.aqr1P0 0 sleep 100 3 running /tmp/ts-out.suaN1K 0 [2]&amp;&amp; sleep 10 4 queued (file) 0 [0,1,3]&amp;&amp; sleep 10 5 queued (file) 0 [0,2,3]&amp;&amp; sleep 10 2 finished /tmp/ts-out.SLCGX7 -1 10.35s 0 sleep 200 . !sleep 100 # let&#39;s wait for jobs 0 and 1 to finish !ts # you will see that the job queued with `-W` will be skipped . ID State Output E-Level Time GPUs Command [run=0/10] 2 finished /tmp/ts-out.SLCGX7 -1 10.35s 0 sleep 200 3 finished /tmp/ts-out.suaN1K 0 10.00s 0 [2]&amp;&amp; sleep 10 0 finished /tmp/ts-out.1wh18P 0 1.67m 0 sleep 100 5 skipped (no output) 0 [0,2,3]&amp;&amp; sleep 10 1 finished /tmp/ts-out.aqr1P0 0 1.67m 0 sleep 100 4 finished /tmp/ts-out.yV8vfT 0 10.00s 0 [0,1,3]&amp;&amp; sleep 10 . To distinguish tasks, you can also label them using the -L flag. . !ts -L foo sleep 10 . 6 . !ts . ID State Output E-Level Time GPUs Command [run=0/10] 2 finished /tmp/ts-out.SLCGX7 -1 10.35s 0 sleep 200 3 finished /tmp/ts-out.suaN1K 0 10.00s 0 [2]&amp;&amp; sleep 10 0 finished /tmp/ts-out.1wh18P 0 1.67m 0 sleep 100 5 skipped (no output) 0 [0,2,7303014]&amp;&amp; sleep 10 1 finished /tmp/ts-out.aqr1P0 0 1.67m 0 sleep 100 4 finished /tmp/ts-out.yV8vfT 0 10.00s 0 [0,1,3]&amp;&amp; sleep 10 6 finished /tmp/ts-out.EO9Qct 0 10.00s 0 [foo]sleep 10 . GPU support . The GPUs column shows the number of GPUs that the task requires. . Before, when running CPU tasks, the number of parallel tasks is capped by the number of slots. For a GPU task, it is further restricted by the number of available GPUs. In other words, a GPU task can run only when there are enough both slots and GPUs. The availability of a GPU is determined by the free memory of that GPU. If more than 90% of the memory is available, the GPU is deemed to be free, and vice versa. If there are more free GPUs than required, the GPUs will be chosen auto-magically and randomly. . There is one thing to note here. Because the availability of a GPU is determined by its memory usage, and it may take time for your task to initialize the GPU memory, so if you run two tasks at the same time, they may use the same device and eventually may crash due to out-of-memory error. Therefore, in Task Spooler, I deliberately delay subsequent GPU tasks a short time (30 seconds by default) after a GPU task is just executed. This is ugly, but it does the job. You can change this delay time via the flag --set_gpu_wait followed by the number of seconds. That&#39;s why when you execute several jobs at once, you may find the tasks after the first one taking a long time to start execution. Also sometimes you may see the job status being changed to running but the task is not actually executed yet, and there is no output file. This is usual. Just keep waiting... It will be executed soon (or sometimes not very soon, but anw it will run)! . Now, to tell Task Spooler that your job requires GPU, use -G followed by the number of required GPUs. Task Spooler will allocate the GPU(s) for the job, and it will make your job see only the provided GPU(s) so your task won&#39;t mess with the others. For a stupid example, let&#39;s sleep with 1 GPU. In your terminal, execute . !ts -K !ts -G 1 sleep 1 !ts . 0 ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.N6RDHT 1 sleep 1 . If you demand more GPUs than available, however, it will queue the task even though there are enough slots. . !ts -G 100 sleep 1 !ts . 1 ID State Output E-Level Time GPUs Command [run=0/1] 1 allocating (file) 100 sleep 1 0 finished /tmp/ts-out.N6RDHT 0 1.00s 1 sleep 1 . In the figure, I demanded 100 GPUs even though the server has only 1, and hence the task has to be queued (in this case, forever). . We havenâ€™t done anything useful yet. In the next section, letâ€™s see how to manage your deep learning experiments using Task Spooler. . Deep learning with Task Spooler . Let&#39;s train a Convolutional Neural Network (CNN) on MNIST. For this example, I will use the official Pytorch MNIST example. To enable the code to use muti-GPU, you will have to manually add . model = nn.DataParallel(model) . after line 124 (optimizer = optim.Adadelta(model.parameters(), lr=args.lr)). You can download the script by executing the cell below. . %%capture !wget https://open-source-codes.s3.amazonaws.com/mnist.py . To train the CNN with Task Spooler using 1 GPU, execute the script as usual in terminal but with ts -G 1 before python. The full command is . !ts -K !ts -G 1 python mnist.py !ts . 0 ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.xwvuBP 1 python mnist.py . Note that without the -G flag, the job will run on CPU instead. . To see the output, use the -c or -t flag. You should see the training in real-time. You can use ctrl+c to stop getting stdout anytime without actually canceling the experiment. . %%capture !ts -t 0 . !ts . ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.xwvuBP 1 python mnist.py . Unfortunately, there is only 1 GPU available in Colab, so I can&#39;t demonstrate training with multiple GPUs. You will have to trust me that it works! . That&#39;s it folks. I hope this little app can boost your productivity and you will enjoy using it for not only your experiments but also your daily tasks. If you have any questions or want to contribute, feel free to create an issue or make a PR on the Github page. . About me . I am Duc Nguyen from Vietnam. Currently, I am a PhD candidate at Yonsei University, Korea. For more information about me, you guys can visit my website or contact me at this email. .",
            "url": "https://librecv.github.io/blog/spooler/task%20manager/deep%20learning/2021/02/09/task-spooler.html",
            "relUrl": "/spooler/task%20manager/deep%20learning/2021/02/09/task-spooler.html",
            "date": " â€¢ Feb 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Kornia - an Open Source Differentiable Computer Vision Library for PyTorch",
            "content": ". What is Kornia? Kornia [1] [2] can be defined as a computer vision library for PyTorch [3], inspired by OpenCV and with strong GPU support. Kornia allows users to write code as if they were using native PyTorch providing high-level interfaces to vision algorithms computed directly on tensors. In addition, some of the main PyTorch features are inherited by Kornia such as a high-performance environment with easy access to automatic differentiation, executing models on different devices (CPU, GPU or Tensor Processing Unit â€” TPU), parallel programming by default, communication primitives for multi-process parallelism across several computation nodes and code ready for production. . . Differentiable. Any image processing algorithm that can be defined as a Direct Acyclic Graph (DAG) structure can be incorporated in a neural network and can be optimized during training, making use of the reverse-mode auto-differentiation, compute gradients via backpropagation. In practice, this means that computer vision functions are operators that can be placed as layers within the neural networks for training via backpropagating through them. . Transparent API. A key component in the library design is its easy way to seamlessly add hardware acceleration to your program with a minimum effort. The library API is agnostic to the input source device, meaning that the algorithms can either be executed in several device types such as CPU, GPU, or the recently introduced TPU. . Parallel programming. Batch processing is another important feature that enables to run vision operators using data parallelism by default. The assumption for the operators is to receive N-channel image tensors as input batches, contrary to standard vision libraries with single 1-3 channel images. Hence, working with multispectral, hyperspectral, or volumetric images can be done in a straight-forward manner using Kornia. . Distributed. Support for communication primitives for multi-process parallelism across several computation nodes running on one or more groups of local or cloud-based machines. The library design allows users to run their applications in different distributed systems, or even able to process large vision pipelines in an efficient way. . Production. Since version v1.0.0, PyTorch has the feature to serialize and optimize models for production purposes. Based on its just-in-time (JIT) compiler, PyTorch traces the models, creating TorchScript programs at runtime in order to be run in a standalone C++ program using kernel fusion to do faster inference. This makes our library a perfect fit also for built-in vision products. . Library Structure . The internal structure of the library is designed to cover different computer vision areas, including color conversions, low-level image processing, geometric transformations, and some utilities for training such as specific loss functions, conversions between data layouts for different frameworks, or functionalities to easily visualize images and debug models during training. . . Component Description . kornia | a Differentiable Computer Vision library like OpenCV, with strong GPU support | . kornia.augmentation | a module to perform data augmentation in the GPU | . kornia.color | a set of routines to perform color space conversions | . kornia.contrib | a compilation of user contrib and experimental operators | . kornia.enhance | a module to perform normalization and intensity transformations | . kornia.feature | a module to perform feature detection | . kornia.filters | a module to perform image filtering and edge detection | . kornia.geometry | a geometric computer vision library to perform image transformations, 3D linear algebra and conversions using different camera models | . kornia.losses | a stack of loss functions to solve different vision tasks | . kornia.morphology | a module to perform morphological operations | . kornia.utils | image to tensor utilities and metrics for vision problems | . You can find a more detailed explanation about the project on the following YouTube video: . Getting started . Kornia is public available in GitHub 1 with an Apache License 2.0 and can be installed in any Linux, MacOS or Windows operating system, having PyTorch as a single dependency, through the Python Package Index (PyPI) using the following command: . %%capture pip install kornia . In the following example we are going to show how to perform a rotation to an image using Kornia and other Python libraries such OpenCV, Numpy and Matplotlib. First, we&#39;ll install this dependencies. . %%capture pip install opencv-python matplotlib . Import the needed package and download an image . import cv2 import numpy as np import matplotlib.pyplot as plt import torch import kornia as K . %%capture !wget -c &quot;https://picsum.photos/200/300&quot; -O &quot;image.png&quot; . Load the image using OpenCV . img: np.ndarray = cv2.imread(&quot;image.png&quot;, cv2.IMREAD_COLOR) # HxWx3 print(f&quot;Numpy image shape: {img.shape}&quot;) # convert the numpy array to a torch tensor img_t: torch.Tensor = K.image_to_tensor(img, keepdim=False) # 1x3xHxW img_t = K.bgr_to_rgb(img_t) img_t = img_t.float() / 255. print(f&quot;Tensor image shape: {img_t.shape}&quot;) . Numpy image shape: (300, 200, 3) Tensor image shape: torch.Size([1, 3, 300, 200]) . Create a tensor with the rotation angle and apply kornia.rotate to the input image tensor . angle: float = 45. # in degrees angle = torch.tensor(angle) # apply the transform to the image img_out: torch.Tensor = K.rotate(img_t, angle) # 1x3xHxW print(f&quot;Output image shape: {img_t.shape}&quot;) . Output image shape: torch.Size([1, 3, 300, 200]) . Display the rotated image using Matplotlib . img_vis: np.ndarray = ( K.tensor_to_image(torch.cat([img_t, img_out], dim=-1)) ) plt.figure(figsize=(10, 10)) plt.imshow(img_vis) plt.axis(&#39;off&#39;) plt.show() . Summary . We have introduced Kornia, a library for computer vision in PyTorch that implements traditional vision algorithms ina differentiable manner making use of hardware acceleration to improve performance. We believe that classical computer vision libraries can take a different role within the deep learning environments as components of layers of the networks as well as pre- and post-processing of the results and change the Computer Vision paradigm. . References . [1]â€œKornia: an Open Source Differentiable Computer Vision Library for PyTorch,â€ in Winter Conference on Applications of Computer Vision, 2020. | [2]E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer, and G. Bradski, â€œA survey on Kornia: an Open Source Differentiable Computer Vision Library for PyTorch,â€ 2020. | [3]A. Paszke et al., â€œPyTorch: An Imperative Style, High-Performance Deep Learning Library,â€ in Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. dâ€™ AlchÃ©-Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc., 2019, pp. 8024â€“8035. | . Footnotes . 1. Kornia GitHub: https://github.com/kornia/kornia!â†© .",
            "url": "https://librecv.github.io/blog/computer%20vision/differentiable%20operators/pytorch/2021/01/27/kornia_library.html",
            "relUrl": "/computer%20vision/differentiable%20operators/pytorch/2021/01/27/kornia_library.html",
            "date": " â€¢ Jan 27, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://librecv.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " â€¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "ghtop redux",
            "content": ". Introduction . We recently refactored the CLI tool ghtop, created by the CEO of GitHub, Nat Friedman. Nat even described our refactor as a â€œtour de forceâ€. This post describes what we learned along the way. . Motivation . Recently, we released ghapi, a new python client for the GitHub API. ghapi provides unparalleled ease of access to the GitHub api, as well as utilities for interacting with GitHub Actions. Part of our motivation for creating ghapi was to accelerate the development of build, testing and deployment tools that help us in maintaining fastai projects. . We recently started using GitHub Actions to perform a wide variety of tasks automatically like: unit and integration tests, deploying documentation, building Docker containers and Conda packages, sharing releases on Twitter, and much more. This automation is key to maintaining the vast open source fastai ecosystem with very few maintainers. . Since ghapi is central to so many of these tasks, we wanted to stress-test its efficacy against other projects. Thatâ€™s when we found ghtop. This tool allows you to stream all the public events happening on GitHub to a CLI dashboard. We thought it would be a fun learning experience to refactor this code base with various fastai tools such as ghapi and fastcore, but also try out new libraries like rich. . Features we added to our tools . While exploring ghtop, we added several features to various fastai tools that we found to be generally useful. . ghapi Authentication . We added the function github_auth_device which allows users to authenticate their api client with GitHub interactively in a browser. When we call this function we get the following prompt: . github_auth_device() . First copy your one-time code: 276E-C910 Then visit https://github.com/login/device in your browser, and paste the code when prompted. Shall we try to open the link for you? [y/n] . The browser opens a window that looks like this: . . The function then returns an authenticated token which you can use for various tasks. While this is not the only way to create a token, this is a user friendly way to create a token, especially for those who are not as familiar with GitHub. . ghapi Events . As a result of our explorations with ghtop, we added an event module to ghapi. This is useful for retrieving and inspecting sample events. Inspecting sample events is important as it allows you to prototype GitHub Actions workflows locally. You can sample real events with load_sample_events: . from ghapi.event import load_sample_events evts = load_sample_events() . Individual events are formatted as markdown lists to be human readable in Jupyter: . print(evts[0]) . - id: 14517925737 - type: PushEvent - actor: - id: 17030246 - login: BeckhamL - display_login: BeckhamL - gravatar_id: - url: https://api.github.com/users/BeckhamL - avatar_url: https://avatars.githubusercontent.com/u/17030246? - repo: - id: 154349747 - name: BeckhamL/leetcode - url: https://api.github.com/repos/BeckhamL/leetcode - payload: - push_id: 6194986903 - size: 1 - distinct_size: 1 - ref: refs/heads/master - head: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - before: cb16921949c969b5153a0c23ce8fe516d2c8d773 - commits: - - sha: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - author: - email: beckham.lam@mail.mcgill.ca - name: Beckham Lam - message: Create detectCapital.ts - distinct: True - url: https://api.github.com/repos/BeckhamL/leetcode/commits/2055b0fcf22f1c3543e38b60199f6882266d32a5 - public: True - created_at: 2020-12-13T21:32:34Z . You can also inspect the json data in an event, which are accessible as attributes: . evts[0].type . &#39;PushEvent&#39; . For example, here is the frequency of all full_types in the sample: . x,y = zip(*Counter([o.full_type for o in evts]).most_common()) plt.figure(figsize=(8, 6)) plt.barh(x[::-1],y[::-1]); . We can fetch public events in parallel with GhApi.list_events_parallel. In our experiments, repeatedly calling list_events_parallel is fast enough to fetch all current public activity from all users across the entire GitHub platform. We use this for ghtop. Behind the scenes, list_events_parallel uses Python&#39;s ThreadPoolExecutor to fetch events in parallel - no fancy distributed systems or complicated infrastructure necessary, even at the scale of GitHub! . %time api = GhApi() evts = api.list_events_parallel() len(evts) . CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs Wall time: 4.29 Âµs . 240 . Note that the GitHub API is stateless, so successive calls to the API will likely return events already seen. We handle this by using a set operations to filter out events already seen. . ghapi pagination . One of the most cumbersome aspects of fetching lots of data from the GitHub api can be pagination. As mentioned in the documentation, different endpoints have different pagination rules and defaults. Therefore, many api clients offer clunky or incomplete interfaces for pagination. . In ghapi we added an entire module with various tools to make paging easier. Below is an example for retrieving repos for the github org. Without pagination, we can only retrieve a fixed number at a time (by default 30): . api = GhApi() repos = api.repos.list_for_org(&#39;fastai&#39;) len(repos) . 30 . However, to get more we can paginate through paged: . from ghapi.event import paged repos = paged(api.repos.list_for_org, &#39;fastai&#39;) for page in repos: print(len(page), page[0].name) . 30 fast-image 30 fastforest 30 .github 8 tweetrel . You can learn more about this functionality by reading the docs. . fastcore Sparklines . Part of goals for refactoring ghtop were to introduce cool visualizations in the terminal of data. We drew inspiration from projects like bashtop, which have CLI interfaces that look like this: . Concretely, we really liked the idea of sparklines in the terminal. Therefore, we created the ability to show sparklines with fastcore: . from fastcore.utils import sparkline data = [9,6,None,1,4,0,8,15,10] print(f&#39;without &quot;empty_zero&quot;: {sparkline(data, empty_zero=False)}&#39;) print(f&#39; with &quot;empty_zero&quot;: {sparkline(data, empty_zero=True )}&#39;) . without &#34;empty_zero&#34;: â–…â–‚ â–â–‚â–â–ƒâ–‡â–… with &#34;empty_zero&#34;: â–…â–‚ â–â–‚ â–ƒâ–‡â–… . For more information on this function, read the docs. Later in this post, we will describe how we used Rich to add color and animation to these sparklines. . fastcore EventTimer . Because we wanted streaming event data to automatically populate sparklines, we created EventTimer that constructs a histogram according to a frequency and time span you set. With EventTimer, you can add events with add, and get the number of events and their frequency: . from fastcore.utils import EventTimer from time import sleep import random def _randwait(): yield from (sleep(random.random()/200) for _ in range(100)) c = EventTimer(store=5, span=0.03) for o in _randwait(): c.add(1) print(f&#39;Num Events: {c.events}, Freq/sec: {c.freq:.01f}&#39;) print(&#39;Most recent: &#39;, sparkline(c.hist), *L(c.hist).map(&#39;{:.01f}&#39;)) . Num Events: 6, Freq/sec: 301.1 Most recent: â–ƒâ–â–â–‡â– 323.6 274.8 291.3 390.9 283.6 . For more information, see the docs. . CLI Animations With Rich . Rich is an amazing python library that allows you to create beautiful, animated and interactive CLI interfaces. Below is a preview of some its features: . Rich also offers animated elements like spinners: . ... and progress bars: . While this post is not about rich, we highly recommend visiting the repo and the docs to learn more. Rich allows you to create your own custom elements. We created two custom elements - Stats and FixedPanel, which we describe below: . Stats: Sparklines with metrics . Stats renders a group of sparklines along with a spinner and a progress bar. First we define our sparklines, the last argument being a list of event types to count: . from ghtop.richext import * from ghtop.all_rich import * console = Console() s1 = ESpark(&#39;Issues&#39;, &#39;green&#39;, [IssueCommentEvent, IssuesEvent]) s2 = ESpark(&#39;PR&#39;, &#39;red&#39;, [PullRequestEvent, PullRequestReviewCommentEvent, PullRequestReviewEvent]) s3 = ESpark(&#39;Follow&#39;, &#39;blue&#39;, [WatchEvent, StarEvent]) s4 = ESpark(&#39;Other&#39;, &#39;red&#39;) s = Stats([s1,s2,s3,s4], store=5, span=.1, stacked=True) console.print(s) . ðŸŒ Issues PR Follow Other Quota /min 0.0 0.0 0.0 0.0 â”â”â”â”â”â”â” 0% . You can add events to update counters and sparklines with add_events: . evts = load_sample_events() s.add_events(evts) console.print(s) . ðŸŒ Issues PR Follow Other Quota /min 11772 â–â–‡ 16546 â–â–‡ 5991 â–â–‡ 6484 â– â”â”â”â”â”â”â” 0% . You can update the progress bar with the update_prog method: . s.update_prog(50) console.print(s) . ðŸŒ Issues PR Follow Other Quota /min 4076 â–â–‡ 5408 â–â–‡ 1834 â–â–‡ 5998 â– â”â”â”â•¸â”â”â” 50% . Here is what the animated version looks like: . . FixedPanel: A panel with fixed height . A key aspect of ghtop is showing events in different panels. We created FixedPanel to allow us to arrange panels in a grid that we can incrementally add events to: . p = FixedPanel(15, box=box.HORIZONTALS, title=&#39;ghtop&#39;) for e in evts: p.append(e) grid([[p,p]]) . â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ghtop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ghtop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ðŸ“ª dependaboâ€¦closed PR #3 oâ€¦herzliâ€¦&quot;Bump â€¦ ðŸ“ª dependaboâ€¦closed PR #3 â€¦herzliâ€¦&quot;Bump â€¦ â­ dongjun13 pushed 1 commiâ€¦dongjun13/2 â­ dongjun13 pushed 1 commâ€¦dongjun13/2 â­ admmonitoâ€¦pushed 1 commiâ€¦admmonitors/tâ€¦ â­ admmonitoâ€¦pushed 1 commâ€¦admmonitors/tâ€¦ â­ randomperâ€¦pushed 1 commiâ€¦randomperson1â€¦ â­ randomperâ€¦pushed 1 commâ€¦randomperson1â€¦ â­ ahocevar pushed 6 commiâ€¦openlayers/opeâ€¦ â­ ahocevar pushed 6 commiâ€¦openlayers/opâ€¦ ðŸ­ arjmoto created branch â€¦arjmoto/redux-â€¦ ðŸ­ arjmoto created branchâ€¦arjmoto/redux-â€¦ ðŸ’¬ stale[botâ€¦created commenâ€¦ironhaâ€¦&quot;This â€¦ ðŸ’¬ stale[botâ€¦created commeâ€¦ironhaâ€¦&quot;This â€¦ â­ commit-b0â€¦pushed 1 commiâ€¦commit-b0t/coâ€¦ â­ commit-b0â€¦pushed 1 commâ€¦commit-b0t/coâ€¦ â­ yakirgot pushed 2 commiâ€¦yakirgot/snake â­ yakirgot pushed 2 commiâ€¦yakirgot/snake ðŸ’¬ awolf78 created commentâ€¦Impulseâ€¦&quot;If yoâ€¦ ðŸ’¬ awolf78 created commenâ€¦Impulseâ€¦&quot;If yoâ€¦ â­ kreus7 pushed 1 commitâ€¦kreus7/kreusadaâ€¦ â­ kreus7 pushed 1 commitâ€¦kreus7/kreusadâ€¦ â­ rgripper pushed 1 commiâ€¦rgripper/webcoâ€¦ â­ rgripper pushed 1 commiâ€¦rgripper/webcâ€¦ ðŸ‘€ thelittleâ€¦started watchiâ€¦ritchie46/polâ€¦ ðŸ‘€ thelittleâ€¦started watchâ€¦ritchie46/polâ€¦ ðŸ­ adrian698 created branchâ€¦adrian698/Test ðŸ­ adrian698 created brancâ€¦adrian698/Test â­ mergify[bâ€¦pushed 2 commiâ€¦spbu-coding/6â€¦ â­ mergify[bâ€¦pushed 2 commâ€¦spbu-coding/6â€¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ . To learn more about our extensions to rich see these docs. . A demo of ghtop animations . Putting all of this together, we get the following results: . 4 Panels with a sparkline for different types of events: . . single panel with a sparkline . . To learn more about ghtop, see the docs. . Interesting python features used . While making these docs, we used the following python features that at least one person we demoed it to found interesting or didn&#39;t know about. If you have been using python for sometime, you might know about all or most of these features: . yield from . Generators are a powerful feature of python, which are especially useful for iterating through large datasets lazily. . dequeue . f-strings .",
            "url": "https://librecv.github.io/blog/ghtop",
            "relUrl": "/ghtop",
            "date": " â€¢ Jan 29, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a â€œlevel 1 headingâ€ in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Hereâ€™s a footnote 1. Hereâ€™s a horizontal rule: . . Lists . Hereâ€™s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes â€¦andâ€¦ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.Â &#8617; . |",
            "url": "https://librecv.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " â€¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Learn more about us in www.librecv.org . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://librecv.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://librecv.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}