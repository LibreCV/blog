{
  
    
        "post0": {
            "title": "Pix2Pix - Image to image translation with Conditional Adversarial Networks",
            "content": "Author Introduction . Hi! My name is Aniket Maurya. I am a Machine Learning Engineer at Quinbay Technologies, India. I research and build ML products. I like to share my limited knowledge of Machine Learning and Deep Learning with on my blog or YouTube channel.You can connect with me on Linkedin/Twitter. . Introduction to Conditional Adversarial Networks . Image to Image translation means transforming the given source image into a different image. Gray scale image to colour image conversion is one such example of image of image translation. . In this tutorial we will discuss GANs, a few points from Pix2Pix paper and implement the Pix2Pix network to translate segmented facade into real pictures. We will create the Pix2Pix model in PyTorch and use PyTorch lightning to avoid boilerplates. . Authors of Image-to-Image Translation with Conditional Adversarial Networks paper has also made the source code publically available on GitHub.&gt; A more detailed tutorial on GANs can be found here - Yann LeCun‚Äôs Deep Learning Course at CDS . GANs are Generative models that learns a mapping from random noise vector (z) to an output image. G(z) -&gt; Image (y) . For example, GANs can learn mapping from random normal vectors to generate smiley images. For training such a GAN we just need a set of smiley images and train the GAN with an adversarial loss üôÇ. After the model is trained we can use random normal noise vectors to generate images that were not in the training dataset. . But what if we want to build a network in such a way that we can control what the model will generate. In our case we want the model to generate a laughing smiley. . Conditional GANs are Generative networks which learn mapping from random noise vectors and a conditional vector to output an image. Suppose we have 4 types of smileys - smile, laugh, sad and angry (üôÇ üòÇ üòî üò°). So our class vector for smile üôÇ can be (1,0,0,0), laugh can be üòÇ (0,1,0,0) and similarly for others. Here the conditional vector is the smiley embedding. . During training of the generator the conditional image is passed to the generator and fake image is generated. The fake image is then passed through the discriminator along with the conditional image, both fake image and conditional image are concatenated. Discriminator penalizes the generator if it correctly classifies the fake image as fake. . Pix2Pix . Pix2Pix is an image-to-image translation Generative Adversarial Networks that learns a mapping from an image X and a random noise Z to output image Y or in simple language it learns to translate the source image into a different distribution of image. . During the time Pix2Pix was released, several other works were also using Conditional GANs on discrete labels. Pix2Pix uses a U-Net based architecture for the Generator and for the Discriminator a PatchGAN Classifier is used. . . Here is what Phillipi has to say about PatchGAN - . Pix2Pix Generator is an U-Net based architecture which is an encoder-decoder network with skip connections. The name U-Net highlights the structure of the &quot;U&quot; shaped network. Both generator and discriminator uses Convolution-BatchNorm-ReLu like module or in simple words we can say that it is the unit block of the generator and discriminator. Skip connections are added between each layer i and layer n‚àíi, where n is the total number of layers. At each skip connection all the channels from current layer i are concatenated with all the channels at n-i layer. . Lets understand it more with code. . import os from glob import glob from pathlib import Path import matplotlib.pyplot as plt import pytorch_lightning as pl import torch from PIL import Image from torch import nn from torch.utils.data import DataLoader, Dataset from torchvision import transforms from torchvision.transforms.functional import center_crop from torchvision.utils import make_grid from tqdm.auto import tqdm . . Uncomment the below code to download the dataset . !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz !tar -xvf facades.tar.gz . After downloading the dataset we create Dataloader which loads our conditional and real image. . path = &quot;./facades/train/&quot; class FacadesDataset(Dataset): def __init__(self, path, target_size=None): self.filenames = glob(str(Path(path) / &quot;*&quot;)) self.target_size = target_size def __len__(self): return len(self.filenames) def __getitem__(self, idx): filename = self.filenames[idx] image = Image.open(filename) image = transforms.functional.to_tensor(image) image_width = image.shape[2] real = image[:, :, : image_width // 2] condition = image[:, :, image_width // 2 :] target_size = self.target_size if target_size: condition = nn.functional.interpolate(condition, size=target_size) real = nn.functional.interpolate(real, size=target_size) return real, condition . In the first part of U-Net shaped network the layer size decreases, we create a DownSampleConv module for this. This module will contain the unit block that we just created ConvBlock. . class DownSampleConv(nn.Module): def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True): &quot;&quot;&quot; Paper details: - C64-C128-C256-C512-C512-C512-C512-C512 - All convolutions are 4√ó4 spatial filters applied with stride 2 - Convolutions in the encoder downsample by a factor of 2 &quot;&quot;&quot; super().__init__() self.activation = activation self.batchnorm = batchnorm self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding) if batchnorm: self.bn = nn.BatchNorm2d(out_channels) if activation: self.act = nn.LeakyReLU(0.2) def forward(self, x): x = self.conv(x) if self.batchnorm: x = self.bn(x) if self.activation: x = self.act(x) return x . Now in the second part the network expands and so we create UpSampleConv . class UpSampleConv(nn.Module): def __init__( self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True, dropout=False ): super().__init__() self.activation = activation self.batchnorm = batchnorm self.dropout = dropout self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding) if batchnorm: self.bn = nn.BatchNorm2d(out_channels) if activation: self.act = nn.ReLU(True) if dropout: self.drop = nn.Dropout2d(0.5) def forward(self, x): x = self.deconv(x) if self.batchnorm: x = self.bn(x) if self.dropout: x = self.drop(x) return x . Now the basic blocks of the Pix2Pix generated is created, we create the generator module. Generator is formed of expanding and contracting layers. The first part network contracts and then expands again, i.e. first we have encoder block and then decoder block. Below is the encoder-decoder of U-Net network configuration from official paper. Here C denotes the unit block that we created ConvBlock and D denotes Drop Out with value 0.5. In the decoder, the output tensors from n-i layer of encoder concatenates with i layer of the decoder. Also the first three blocks of the decoder has drop out layers. . Encoder: C64-C128-C256-C512-C512-C512-C512-C512 Decoder: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128 . class Generator(nn.Module): def __init__(self, in_channels, out_channels): &quot;&quot;&quot; Paper details: - Encoder: C64-C128-C256-C512-C512-C512-C512-C512 - All convolutions are 4√ó4 spatial filters applied with stride 2 - Convolutions in the encoder downsample by a factor of 2 - Decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128 &quot;&quot;&quot; super().__init__() # encoder/donwsample convs self.encoders = [ DownSampleConv(in_channels, 64, batchnorm=False), # bs x 64 x 128 x 128 DownSampleConv(64, 128), # bs x 128 x 64 x 64 DownSampleConv(128, 256), # bs x 256 x 32 x 32 DownSampleConv(256, 512), # bs x 512 x 16 x 16 DownSampleConv(512, 512), # bs x 512 x 8 x 8 DownSampleConv(512, 512), # bs x 512 x 4 x 4 DownSampleConv(512, 512), # bs x 512 x 2 x 2 DownSampleConv(512, 512, batchnorm=False), # bs x 512 x 1 x 1 ] # decoder/upsample convs self.decoders = [ UpSampleConv(512, 512, dropout=True), # bs x 512 x 2 x 2 UpSampleConv(1024, 512, dropout=True), # bs x 512 x 4 x 4 UpSampleConv(1024, 512, dropout=True), # bs x 512 x 8 x 8 UpSampleConv(1024, 512), # bs x 512 x 16 x 16 UpSampleConv(1024, 256), # bs x 256 x 32 x 32 UpSampleConv(512, 128), # bs x 128 x 64 x 64 UpSampleConv(256, 64), # bs x 64 x 128 x 128 ] self.decoder_channels = [512, 512, 512, 512, 256, 128, 64] self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1) self.tanh = nn.Tanh() self.encoders = nn.ModuleList(self.encoders) self.decoders = nn.ModuleList(self.decoders) def forward(self, x): skips_cons = [] for encoder in self.encoders: x = encoder(x) skips_cons.append(x) skips_cons = list(reversed(skips_cons[:-1])) decoders = self.decoders[:-1] for decoder, skip in zip(decoders, skips_cons): x = decoder(x) # print(x.shape, skip.shape) x = torch.cat((x, skip), axis=1) x = self.decoders[-1](x) # print(x.shape) x = self.final_conv(x) return self.tanh(x) . Discriminator . A discriminator is a ConvNet which learns to classify images into discrete labels. In GANs, discriminators learns to predict whether the given image is real or fake. PatchGAN is the discriminator used for Pix2Pix. Its architecture is different from a typical image classification ConvNet because of the output layer size. In convnets output layer size is equal to the number of classes while in PatchGAN output layer size is a 2D matrix. . Now we create our Discriminator - PatchGAN. In this network we use the same DownSampleConv module that we created for generator. . class PatchGAN(nn.Module): def __init__(self, input_channels): super().__init__() self.d1 = DownSampleConv(input_channels, 64, batchnorm=False) self.d2 = DownSampleConv(64, 128) self.d3 = DownSampleConv(128, 256) self.d4 = DownSampleConv(256, 512) self.final = nn.Conv2d(512, 1, kernel_size=1) def forward(self, x, y): x = torch.cat([x, y], axis=1) x0 = self.d1(x) x1 = self.d2(x0) x2 = self.d3(x1) x3 = self.d4(x2) xn = self.final(x3) return xn . Loss Function . Loss function used in Pix2Pix are Adversarial loss and Reconstruction loss. Adversarial loss is used to penalize the generator to predict more realistic images. In conditional GANs, generators job is not only to produce realistic image but also to be near the ground truth output. Reconstruction Loss helps network to produce the realistic image near the conditional image. . adversarial_loss = nn.BCEWithLogitsLoss() reconstruction_loss = nn.L1Loss() . # https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch def _weights_init(m): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): torch.nn.init.normal_(m.weight, 0.0, 0.02) if isinstance(m, nn.BatchNorm2d): torch.nn.init.normal_(m.weight, 0.0, 0.02) torch.nn.init.constant_(m.bias, 0) def display_progress(cond, fake, real, figsize=(10,5)): cond = cond.detach().cpu().permute(1, 2, 0) fake = fake.detach().cpu().permute(1, 2, 0) real = real.detach().cpu().permute(1, 2, 0) fig, ax = plt.subplots(1, 3, figsize=figsize) ax[0].imshow(cond) ax[2].imshow(fake) ax[1].imshow(real) plt.show() . . class Pix2Pix(pl.LightningModule): def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, display_step=25): super().__init__() self.save_hyperparameters() self.display_step = display_step self.gen = Generator(in_channels, out_channels) self.patch_gan = PatchGAN(in_channels + out_channels) # intializing weights self.gen = self.gen.apply(_weights_init) self.patch_gan = self.patch_gan.apply(_weights_init) self.adversarial_criterion = nn.BCEWithLogitsLoss() self.recon_criterion = nn.L1Loss() def _gen_step(self, real_images, conditioned_images): # Pix2Pix has adversarial and a reconstruction loss # First calculate the adversarial loss fake_images = self.gen(conditioned_images) disc_logits = self.patch_gan(fake_images, conditioned_images) adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits)) # calculate reconstruction loss recon_loss = self.recon_criterion(fake_images, real_images) lambda_recon = self.hparams.lambda_recon return adversarial_loss + lambda_recon * recon_loss def _disc_step(self, real_images, conditioned_images): fake_images = self.gen(conditioned_images).detach() fake_logits = self.patch_gan(fake_images, conditioned_images) real_logits = self.patch_gan(real_images, conditioned_images) fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits)) real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits)) return (real_loss + fake_loss) / 2 def configure_optimizers(self): lr = self.hparams.learning_rate gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr) disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr) return disc_opt, gen_opt def training_step(self, batch, batch_idx, optimizer_idx): real, condition = batch loss = None if optimizer_idx == 0: loss = self._disc_step(real, condition) self.log(&#39;PatchGAN Loss&#39;, loss) elif optimizer_idx == 1: loss = self._gen_step(real, condition) self.log(&#39;Generator Loss&#39;, loss) if self.current_epoch%self.display_step==0 and batch_idx==0 and optimizer_idx==1: fake = self.gen(condition).detach() display_progress(condition[0], fake[0], real[0]) return loss . Now that the network is implemented now we are ready to train. You can also modify the dataloader and train on custom dataset. . Recently I contributed to Pytorch Lightning and now the Pix2Pix model is also available on Pytorch Lightning Bolts, feel free to try that out as well. Hope you liked the article! Happy training üòÉ . dataset = FacadesDataset(path, target_size=target_size) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True) pix2pix = Pix2Pix(3, 3, learning_rate=lr, lambda_recon=lambda_recon, display_step=display_step) trainer = pl.Trainer(max_epochs=1000, gpus=1) trainer.fit(pix2pix, dataloader) . GPU available: True, used: True TPU available: None, using: 0 TPU cores | Name | Type | Params 0 | gen | Generator | 54.4 M 1 | patch_gan | PatchGAN | 2.8 M 2 | adversarial_criterion | BCEWithLogitsLoss | 0 3 | recon_criterion | L1Loss | 0 57.2 M Trainable params 0 Non-trainable params 57.2 M Total params 228.713 Total estimated model params size (MB) Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). . Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). .",
            "url": "https://librecv.github.io/blog/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html",
            "relUrl": "/gans/pytorch/2021/02/13/Pix2Pix-explained-with-code.html",
            "date": " ‚Ä¢ Feb 13, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep Learning Experiments with Task Spooler",
            "content": "Introduction . Task Spooler was originally developed by Lluis Batlle i Rossell but is no longer maintained. The branch introduced here is a fork of the original program with more features including GPU support. . Installation . First, you can clone Task Spooler from Github. Optionally, you can choose a different version by checking out another tag. In this tutorial, I will use the latest version on master. . %%capture !git clone https://github.com/justanhduc/task-spooler . Next, you need to create a CUDA_HOME environment variable to point to the CUDA root directory. Then, you can execute the given install script. . !cd task-spooler/ &amp;&amp; CUDA_HOME=/usr/local/cuda ./reinstall . rm -f *.o ts cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c main.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c server.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c server_start.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c client.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c msgdump.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c jobs.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c execute.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c msg.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c mail.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c error.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c signals.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c list.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c print.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c info.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c env.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -c tail.c cc -pedantic -ansi -Wall -g -O0 -std=c11 -D_XOPEN_SOURCE=500 -D__STRICT_ANSI__ -L/usr/local/cuda/lib64 -I/usr/local/cuda/include -lpthread -c gpu.c In file included from gpu.c:6:0: /usr/local/cuda/include/nvml.h:6208:51: warning: ISO C restricts enumerator values to range of ‚Äòint‚Äô [-Wpedantic] NVML_VGPU_COMPATIBILITY_LIMIT_OTHER = 0x80000000, //!&lt; Compatibility is limited by an undefined factor. ^~~~~~~~~~ cc -o ts main.o server.o server_start.o client.o msgdump.o jobs.o execute.o msg.o mail.o error.o signals.o list.o print.o info.o env.o tail.o gpu.o -L/usr/local/cuda/lib64 -L/usr/local/cuda/lib64/stubs -I/usr/local/cuda/include -lpthread -lcudart -lcublas -fopenmp -lnvidia-ml make: &#39;uninstall&#39; is up to date. install -c -d /usr/local/bin install -c ts /usr/local/bin install -c -d /usr/local/share/man/man1 install -c -m 644 ts.1 /usr/local/share/man/man1 . Basics of Task Spooler . First look . !ts . ID State Output E-Level Time GPUs Command [run=0/1] . The interface of Task Spooler can be seen like above by simply executing ts without argument. In the figure above, ID refers to job ID. There are four main types of State: running indicates that a job is currently running, queued that a CPU job is waiting to be executed, allocating is a queued GPU job, and running means the job is currently being executed. When a job is executed, the stdout stream is redirected to a file under the Output tab. These log files will never automatically deleted even after the job list is cleared. E-Level captures and displays the return error of a process. Time indicates the running time of a job. The running command is shown in the Command column. The numbers inside the square bracket next to Command specify the number of currently running jobs and the maximum jobs (slots) that can be run in parallel. For example, in the figure above, there is no running job and you can run at most one job in parallel, respectively. The maximum slot number can be adjusted manually. . Queuing your first job . Jobs can be added by simply appending ts in front of your command. For e.g., to run make the system sleep for 10 seconds using Task Spooler, execute . !ts sleep 10 !ts !sleep 10 # lets check ts again after 10 seconds !ts . 0 ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.j0MGwO 0 sleep 10 ID State Output E-Level Time GPUs Command [run=0/1] 0 finished /tmp/ts-out.j0MGwO 0 10.00s 0 sleep 10 . You can see that the first job with ID 0 is currently running, and the other job is being queued. After 10 seconds, the first job will finish with an E-Level of 0 and the second job will start. . To enable running more jobs in parallel, you can increase the maximum slot number by using a -S flag followed by the desired number. For instance, . !ts -S 4 !ts . ID State Output E-Level Time GPUs Command [run=0/4] 0 finished /tmp/ts-out.j0MGwO 0 10.00s 0 sleep 10 . The command above allows you to run 4 jobs at the same time. You can verify by typing ts and the last number in the square bracket should change to 4. Let&#39;s try queuing 5 jobs at once and this time we should increase the sleep time to 100 so that the job doesn&#39;t end too fast. You should be able to see something like this . !ts sleep 100 !ts sleep 20 !ts sleep 30 !ts sleep 40 !ts sleep 10 !ts . 1 2 3 4 5 ID State Output E-Level Time GPUs Command [run=4/4] 1 running /tmp/ts-out.xDq00e 0 sleep 100 2 running /tmp/ts-out.HUzUai 0 sleep 20 3 running /tmp/ts-out.sYcGno 0 sleep 30 4 running /tmp/ts-out.ArV4nv 0 sleep 40 5 queued (file) 0 sleep 10 0 finished /tmp/ts-out.j0MGwO 0 10.00s 0 sleep 10 . Viewing command outputs . As mentioned above, the stdout of the command is redirected to a file specified in the Output column. To manually see the written output, you can simply look for that file. But of course Task Spooler is more than that. It lets you read the outputs contents in two ways via the flags -t and -c. . -c, which stands for cat, allows you to see all the output from the beginning to the end. -t, which means tail, displays only the last 10 lines of the output. Let&#39;s try them out. First, we can something that can produce a lot of texts, like ls, df or du. The choice is yours. For me, I ran ts ls /usr/bin. The job ID of the command in my case is 0 so to visualize the whole output, I used ts -c 0. It displayed a long list of excutable files. When I typed ts -t 0, it showed only the last 10 lines. . !ts -K # reset Task Spooler. it will be introduced later !ts ls /usr/bin !ts -t 0 . 0 yes zdump zip zipcloak zipdetails zipgrep zipinfo zipnote zipsplit zrun . . %%capture !ts -c 0 . Miscs . There are many other flag options to manage your tasks. First of all, to see all the available options, use a -h options. Among these, the ones you probably will use most are -r, -C, -k, -T and -K. To remove a queued or finished job (with finished, queued or allocating status), use -r with optionally a job ID. For example, ts -r removes the last added job if it is not running yet. ts -r 10 removes the job with ID 10. If the job is successfully removed, it should disappear from the job list. . !ts -K !ts -S 2 # lets run 2 tasks at a time !ts sleep 100 !ts sleep 100 !ts sleep 100 !ts . 0 1 2 ID State Output E-Level Time GPUs Command [run=2/2] 0 running /tmp/ts-out.gClvpl 0 sleep 100 1 running /tmp/ts-out.rW9nIv 0 sleep 100 2 queued (file) 0 sleep 100 . !ts -r 2 # remove job 2 !ts . ID State Output E-Level Time GPUs Command [run=2/2] 0 running /tmp/ts-out.gClvpl 0 sleep 100 1 running /tmp/ts-out.rW9nIv 0 sleep 100 . To kill a running job, use ts -k &lt;jobid&gt;. . !ts -k 0 # lets kill job 0 !ts . ID State Output E-Level Time GPUs Command [run=1/2] 1 running /tmp/ts-out.rW9nIv 0 sleep 100 0 finished /tmp/ts-out.gClvpl -1 8.07s 0 sleep 100 . !ts -S 5 !ts sleep 100 !ts sleep 100 !ts sleep 100 !ts . 3 4 5 ID State Output E-Level Time GPUs Command [run=4/5] 1 running /tmp/ts-out.rW9nIv 0 sleep 100 3 running /tmp/ts-out.BeUKip 0 sleep 100 4 running /tmp/ts-out.uFu50z 0 sleep 100 5 running /tmp/ts-out.o0hd1F 0 sleep 100 0 finished /tmp/ts-out.gClvpl -1 8.07s 0 sleep 100 . To kill all running jobs, use ts -T. . !ts -T # terminates all running jobs !ts . ID State Output E-Level Time GPUs Command [run=0/5] 0 finished /tmp/ts-out.gClvpl -1 8.07s 0 sleep 100 1 finished /tmp/ts-out.rW9nIv -1 22.42s 0 sleep 100 5 finished /tmp/ts-out.o0hd1F -1 8.84s 0 sleep 100 3 finished /tmp/ts-out.BeUKip -1 9.06s 0 sleep 100 4 finished /tmp/ts-out.uFu50z -1 8.95s 0 sleep 100 . To clear all the finished jobs from the list, use -C without argument. . !ts sleep 100 !ts -C # clear job list !ts . 6 ID State Output E-Level Time GPUs Command [run=1/5] 6 running /tmp/ts-out.bOY0Sx 0 sleep 100 . Finally, ts -K will kill the Task Spooler process. . !ts -K # lets kill Task Spooler !ts # then restarts . ID State Output E-Level Time GPUs Command [run=0/1] . There are some useful flags when scheduling tasks as well. You may want to execute a task only after a certain job finishes. In this case you can use the flag -d with no argument to make your future task depend on the last added job, -D with a comma separated list of job IDs which are the IDs of the jobs that the to-be-run task depends on, and -W followed by a list of IDs, which states that the dependent job will run iff all the dependencies finish with exit code 0. For example, . !ts -S 10 # lets queue 3 jobs first !ts sleep 100 !ts sleep 100 !ts sleep 200 !ts . 0 1 2 ID State Output E-Level Time GPUs Command [run=3/10] 0 running /tmp/ts-out.1wh18P 0 sleep 100 1 running /tmp/ts-out.aqr1P0 0 sleep 100 2 running /tmp/ts-out.SLCGX7 0 sleep 200 . !ts -d sleep 10 # does not care about exit code !ts -D 0,1,3 sleep 10 # runs after jobs 0, 1 and 3 !ts -W 0,2,3 sleep 10 # to run this job, jobs 0, 2 and 3 need to finish well !ts . 3 4 5 ID State Output E-Level Time GPUs Command [run=3/10] 0 running /tmp/ts-out.1wh18P 0 sleep 100 1 running /tmp/ts-out.aqr1P0 0 sleep 100 2 running /tmp/ts-out.SLCGX7 0 sleep 200 3 queued (file) 0 [2]&amp;&amp; sleep 10 4 queued (file) 0 [0,1,3]&amp;&amp; sleep 10 5 queued (file) 0 [0,2,3]&amp;&amp; sleep 10 . !ts -k 2 !ts . ID State Output E-Level Time GPUs Command [run=3/10] 0 running /tmp/ts-out.1wh18P 0 sleep 100 1 running /tmp/ts-out.aqr1P0 0 sleep 100 3 running /tmp/ts-out.suaN1K 0 [2]&amp;&amp; sleep 10 4 queued (file) 0 [0,1,3]&amp;&amp; sleep 10 5 queued (file) 0 [0,2,3]&amp;&amp; sleep 10 2 finished /tmp/ts-out.SLCGX7 -1 10.35s 0 sleep 200 . !sleep 100 # let&#39;s wait for jobs 0 and 1 to finish !ts # you will see that the job queued with `-W` will be skipped . ID State Output E-Level Time GPUs Command [run=0/10] 2 finished /tmp/ts-out.SLCGX7 -1 10.35s 0 sleep 200 3 finished /tmp/ts-out.suaN1K 0 10.00s 0 [2]&amp;&amp; sleep 10 0 finished /tmp/ts-out.1wh18P 0 1.67m 0 sleep 100 5 skipped (no output) 0 [0,2,3]&amp;&amp; sleep 10 1 finished /tmp/ts-out.aqr1P0 0 1.67m 0 sleep 100 4 finished /tmp/ts-out.yV8vfT 0 10.00s 0 [0,1,3]&amp;&amp; sleep 10 . To distinguish tasks, you can also label them using the -L flag. . !ts -L foo sleep 10 . 6 . !ts . ID State Output E-Level Time GPUs Command [run=0/10] 2 finished /tmp/ts-out.SLCGX7 -1 10.35s 0 sleep 200 3 finished /tmp/ts-out.suaN1K 0 10.00s 0 [2]&amp;&amp; sleep 10 0 finished /tmp/ts-out.1wh18P 0 1.67m 0 sleep 100 5 skipped (no output) 0 [0,2,7303014]&amp;&amp; sleep 10 1 finished /tmp/ts-out.aqr1P0 0 1.67m 0 sleep 100 4 finished /tmp/ts-out.yV8vfT 0 10.00s 0 [0,1,3]&amp;&amp; sleep 10 6 finished /tmp/ts-out.EO9Qct 0 10.00s 0 [foo]sleep 10 . GPU support . The GPUs column shows the number of GPUs that the task requires. . Before, when running CPU tasks, the number of parallel tasks is capped by the number of slots. For a GPU task, it is further restricted by the number of available GPUs. In other words, a GPU task can run only when there are enough both slots and GPUs. The availability of a GPU is determined by the free memory of that GPU. If more than 90% of the memory is available, the GPU is deemed to be free, and vice versa. If there are more free GPUs than required, the GPUs will be chosen auto-magically and randomly. . There is one thing to note here. Because the availability of a GPU is determined by its memory usage, and it may take time for your task to initialize the GPU memory, so if you run two tasks at the same time, they may use the same device and eventually may crash due to out-of-memory error. Therefore, in Task Spooler, I deliberately delay subsequent GPU tasks a short time (30 seconds by default) after a GPU task is just executed. This is ugly, but it does the job. You can change this delay time via the flag --set_gpu_wait followed by the number of seconds. That&#39;s why when you execute several jobs at once, you may find the tasks after the first one taking a long time to start execution. Also sometimes you may see the job status being changed to running but the task is not actually executed yet, and there is no output file. This is usual. Just keep waiting... It will be executed soon (or sometimes not very soon, but anw it will run)! . Now, to tell Task Spooler that your job requires GPU, use -G followed by the number of required GPUs. Task Spooler will allocate the GPU(s) for the job, and it will make your job see only the provided GPU(s) so your task won&#39;t mess with the others. For a stupid example, let&#39;s sleep with 1 GPU. In your terminal, execute . !ts -K !ts -G 1 sleep 1 !ts . 0 ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.N6RDHT 1 sleep 1 . If you demand more GPUs than available, however, it will queue the task even though there are enough slots. . !ts -G 100 sleep 1 !ts . 1 ID State Output E-Level Time GPUs Command [run=0/1] 1 allocating (file) 100 sleep 1 0 finished /tmp/ts-out.N6RDHT 0 1.00s 1 sleep 1 . In the figure, I demanded 100 GPUs even though the server has only 1, and hence the task has to be queued (in this case, forever). . We haven‚Äôt done anything useful yet. In the next section, let‚Äôs see how to manage your deep learning experiments using Task Spooler. . Deep learning with Task Spooler . Let&#39;s train a Convolutional Neural Network (CNN) on MNIST. For this example, I will use the official Pytorch MNIST example. To enable the code to use muti-GPU, you will have to manually add . model = nn.DataParallel(model) . after line 124 (optimizer = optim.Adadelta(model.parameters(), lr=args.lr)). You can download the script by executing the cell below. . %%capture !wget https://open-source-codes.s3.amazonaws.com/mnist.py . To train the CNN with Task Spooler using 1 GPU, execute the script as usual in terminal but with ts -G 1 before python. The full command is . !ts -K !ts -G 1 python mnist.py !ts . 0 ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.xwvuBP 1 python mnist.py . Note that without the -G flag, the job will run on CPU instead. . To see the output, use the -c or -t flag. You should see the training in real-time. You can use ctrl+c to stop getting stdout anytime without actually canceling the experiment. . %%capture !ts -t 0 . !ts . ID State Output E-Level Time GPUs Command [run=1/1] 0 running /tmp/ts-out.xwvuBP 1 python mnist.py . Unfortunately, there is only 1 GPU available in Colab, so I can&#39;t demonstrate training with multiple GPUs. You will have to trust me that it works! . That&#39;s it folks. I hope this little app can boost your productivity and you will enjoy using it for not only your experiments but also your daily tasks. If you have any questions or want to contribute, feel free to create an issue or make a PR on the Github page. . About me . I am Duc Nguyen from Vietnam. Currently, I am a PhD candidate at Yonsei University, Korea. For more information about me, you guys can visit my website or contact me at this email. .",
            "url": "https://librecv.github.io/blog/spooler/task%20manager/deep%20learning/2021/02/09/task-spooler.html",
            "relUrl": "/spooler/task%20manager/deep%20learning/2021/02/09/task-spooler.html",
            "date": " ‚Ä¢ Feb 9, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Kornia - an Open Source Differentiable Computer Vision Library for PyTorch",
            "content": ". What is Kornia? Kornia [1] [2] can be defined as a computer vision library for PyTorch [3], inspired by OpenCV and with strong GPU support. Kornia allows users to write code as if they were using native PyTorch providing high-level interfaces to vision algorithms computed directly on tensors. In addition, some of the main PyTorch features are inherited by Kornia such as a high-performance environment with easy access to automatic differentiation, executing models on different devices (CPU, GPU or Tensor Processing Unit ‚Äî TPU), parallel programming by default, communication primitives for multi-process parallelism across several computation nodes and code ready for production. . . Differentiable. Any image processing algorithm that can be defined as a Direct Acyclic Graph (DAG) structure can be incorporated in a neural network and can be optimized during training, making use of the reverse-mode auto-differentiation, compute gradients via backpropagation. In practice, this means that computer vision functions are operators that can be placed as layers within the neural networks for training via backpropagating through them. . Transparent API. A key component in the library design is its easy way to seamlessly add hardware acceleration to your program with a minimum effort. The library API is agnostic to the input source device, meaning that the algorithms can either be executed in several device types such as CPU, GPU, or the recently introduced TPU. . Parallel programming. Batch processing is another important feature that enables to run vision operators using data parallelism by default. The assumption for the operators is to receive N-channel image tensors as input batches, contrary to standard vision libraries with single 1-3 channel images. Hence, working with multispectral, hyperspectral, or volumetric images can be done in a straight-forward manner using Kornia. . Distributed. Support for communication primitives for multi-process parallelism across several computation nodes running on one or more groups of local or cloud-based machines. The library design allows users to run their applications in different distributed systems, or even able to process large vision pipelines in an efficient way. . Production. Since version v1.0.0, PyTorch has the feature to serialize and optimize models for production purposes. Based on its just-in-time (JIT) compiler, PyTorch traces the models, creating TorchScript programs at runtime in order to be run in a standalone C++ program using kernel fusion to do faster inference. This makes our library a perfect fit also for built-in vision products. . Library Structure . The internal structure of the library is designed to cover different computer vision areas, including color conversions, low-level image processing, geometric transformations, and some utilities for training such as specific loss functions, conversions between data layouts for different frameworks, or functionalities to easily visualize images and debug models during training. . . Component Description . kornia | a Differentiable Computer Vision library like OpenCV, with strong GPU support | . kornia.augmentation | a module to perform data augmentation in the GPU | . kornia.color | a set of routines to perform color space conversions | . kornia.contrib | a compilation of user contrib and experimental operators | . kornia.enhance | a module to perform normalization and intensity transformations | . kornia.feature | a module to perform feature detection | . kornia.filters | a module to perform image filtering and edge detection | . kornia.geometry | a geometric computer vision library to perform image transformations, 3D linear algebra and conversions using different camera models | . kornia.losses | a stack of loss functions to solve different vision tasks | . kornia.morphology | a module to perform morphological operations | . kornia.utils | image to tensor utilities and metrics for vision problems | . You can find a more detailed explanation about the project on the following YouTube video: . Getting started . Kornia is public available in GitHub 1 with an Apache License 2.0 and can be installed in any Linux, MacOS or Windows operating system, having PyTorch as a single dependency, through the Python Package Index (PyPI) using the following command: . %%capture pip install kornia . In the following example we are going to show how to perform a rotation to an image using Kornia and other Python libraries such OpenCV, Numpy and Matplotlib. First, we&#39;ll install this dependencies. . %%capture pip install opencv-python matplotlib . Import the needed package and download an image . import cv2 import numpy as np import matplotlib.pyplot as plt import torch import kornia as K . %%capture !wget -c &quot;https://picsum.photos/200/300&quot; -O &quot;image.png&quot; . Load the image using OpenCV . img: np.ndarray = cv2.imread(&quot;image.png&quot;, cv2.IMREAD_COLOR) # HxWx3 print(f&quot;Numpy image shape: {img.shape}&quot;) # convert the numpy array to a torch tensor img_t: torch.Tensor = K.image_to_tensor(img, keepdim=False) # 1x3xHxW img_t = K.bgr_to_rgb(img_t) img_t = img_t.float() / 255. print(f&quot;Tensor image shape: {img_t.shape}&quot;) . Numpy image shape: (300, 200, 3) Tensor image shape: torch.Size([1, 3, 300, 200]) . Create a tensor with the rotation angle and apply kornia.rotate to the input image tensor . angle: float = 45. # in degrees angle = torch.tensor(angle) # apply the transform to the image img_out: torch.Tensor = K.rotate(img_t, angle) # 1x3xHxW print(f&quot;Output image shape: {img_t.shape}&quot;) . Output image shape: torch.Size([1, 3, 300, 200]) . Display the rotated image using Matplotlib . img_vis: np.ndarray = ( K.tensor_to_image(torch.cat([img_t, img_out], dim=-1)) ) plt.figure(figsize=(10, 10)) plt.imshow(img_vis) plt.axis(&#39;off&#39;) plt.show() . Summary . We have introduced Kornia, a library for computer vision in PyTorch that implements traditional vision algorithms ina differentiable manner making use of hardware acceleration to improve performance. We believe that classical computer vision libraries can take a different role within the deep learning environments as components of layers of the networks as well as pre- and post-processing of the results and change the Computer Vision paradigm. . References . [1]‚ÄúKornia: an Open Source Differentiable Computer Vision Library for PyTorch,‚Äù in Winter Conference on Applications of Computer Vision, 2020. | [2]E. Riba, D. Mishkin, J. Shi, D. Ponsa, F. Moreno-Noguer, and G. Bradski, ‚ÄúA survey on Kornia: an Open Source Differentiable Computer Vision Library for PyTorch,‚Äù 2020. | [3]A. Paszke et al., ‚ÄúPyTorch: An Imperative Style, High-Performance Deep Learning Library,‚Äù in Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d‚Äô Alch√©-Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc., 2019, pp. 8024‚Äì8035. | . Footnotes . 1. Kornia GitHub: https://github.com/kornia/kornia!‚Ü© .",
            "url": "https://librecv.github.io/blog/computer%20vision/differentiable%20operators/pytorch/2021/01/27/kornia_library.html",
            "relUrl": "/computer%20vision/differentiable%20operators/pytorch/2021/01/27/kornia_library.html",
            "date": " ‚Ä¢ Jan 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://librecv.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "ghtop redux",
            "content": ". Introduction . We recently refactored the CLI tool ghtop, created by the CEO of GitHub, Nat Friedman. Nat even described our refactor as a ‚Äútour de force‚Äù. This post describes what we learned along the way. . Motivation . Recently, we released ghapi, a new python client for the GitHub API. ghapi provides unparalleled ease of access to the GitHub api, as well as utilities for interacting with GitHub Actions. Part of our motivation for creating ghapi was to accelerate the development of build, testing and deployment tools that help us in maintaining fastai projects. . We recently started using GitHub Actions to perform a wide variety of tasks automatically like: unit and integration tests, deploying documentation, building Docker containers and Conda packages, sharing releases on Twitter, and much more. This automation is key to maintaining the vast open source fastai ecosystem with very few maintainers. . Since ghapi is central to so many of these tasks, we wanted to stress-test its efficacy against other projects. That‚Äôs when we found ghtop. This tool allows you to stream all the public events happening on GitHub to a CLI dashboard. We thought it would be a fun learning experience to refactor this code base with various fastai tools such as ghapi and fastcore, but also try out new libraries like rich. . Features we added to our tools . While exploring ghtop, we added several features to various fastai tools that we found to be generally useful. . ghapi Authentication . We added the function github_auth_device which allows users to authenticate their api client with GitHub interactively in a browser. When we call this function we get the following prompt: . github_auth_device() . First copy your one-time code: 276E-C910 Then visit https://github.com/login/device in your browser, and paste the code when prompted. Shall we try to open the link for you? [y/n] . The browser opens a window that looks like this: . . The function then returns an authenticated token which you can use for various tasks. While this is not the only way to create a token, this is a user friendly way to create a token, especially for those who are not as familiar with GitHub. . ghapi Events . As a result of our explorations with ghtop, we added an event module to ghapi. This is useful for retrieving and inspecting sample events. Inspecting sample events is important as it allows you to prototype GitHub Actions workflows locally. You can sample real events with load_sample_events: . from ghapi.event import load_sample_events evts = load_sample_events() . Individual events are formatted as markdown lists to be human readable in Jupyter: . print(evts[0]) . - id: 14517925737 - type: PushEvent - actor: - id: 17030246 - login: BeckhamL - display_login: BeckhamL - gravatar_id: - url: https://api.github.com/users/BeckhamL - avatar_url: https://avatars.githubusercontent.com/u/17030246? - repo: - id: 154349747 - name: BeckhamL/leetcode - url: https://api.github.com/repos/BeckhamL/leetcode - payload: - push_id: 6194986903 - size: 1 - distinct_size: 1 - ref: refs/heads/master - head: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - before: cb16921949c969b5153a0c23ce8fe516d2c8d773 - commits: - - sha: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - author: - email: beckham.lam@mail.mcgill.ca - name: Beckham Lam - message: Create detectCapital.ts - distinct: True - url: https://api.github.com/repos/BeckhamL/leetcode/commits/2055b0fcf22f1c3543e38b60199f6882266d32a5 - public: True - created_at: 2020-12-13T21:32:34Z . You can also inspect the json data in an event, which are accessible as attributes: . evts[0].type . &#39;PushEvent&#39; . For example, here is the frequency of all full_types in the sample: . x,y = zip(*Counter([o.full_type for o in evts]).most_common()) plt.figure(figsize=(8, 6)) plt.barh(x[::-1],y[::-1]); . We can fetch public events in parallel with GhApi.list_events_parallel. In our experiments, repeatedly calling list_events_parallel is fast enough to fetch all current public activity from all users across the entire GitHub platform. We use this for ghtop. Behind the scenes, list_events_parallel uses Python&#39;s ThreadPoolExecutor to fetch events in parallel - no fancy distributed systems or complicated infrastructure necessary, even at the scale of GitHub! . %time api = GhApi() evts = api.list_events_parallel() len(evts) . CPU times: user 2 ¬µs, sys: 0 ns, total: 2 ¬µs Wall time: 4.29 ¬µs . 240 . Note that the GitHub API is stateless, so successive calls to the API will likely return events already seen. We handle this by using a set operations to filter out events already seen. . ghapi pagination . One of the most cumbersome aspects of fetching lots of data from the GitHub api can be pagination. As mentioned in the documentation, different endpoints have different pagination rules and defaults. Therefore, many api clients offer clunky or incomplete interfaces for pagination. . In ghapi we added an entire module with various tools to make paging easier. Below is an example for retrieving repos for the github org. Without pagination, we can only retrieve a fixed number at a time (by default 30): . api = GhApi() repos = api.repos.list_for_org(&#39;fastai&#39;) len(repos) . 30 . However, to get more we can paginate through paged: . from ghapi.event import paged repos = paged(api.repos.list_for_org, &#39;fastai&#39;) for page in repos: print(len(page), page[0].name) . 30 fast-image 30 fastforest 30 .github 8 tweetrel . You can learn more about this functionality by reading the docs. . fastcore Sparklines . Part of goals for refactoring ghtop were to introduce cool visualizations in the terminal of data. We drew inspiration from projects like bashtop, which have CLI interfaces that look like this: . Concretely, we really liked the idea of sparklines in the terminal. Therefore, we created the ability to show sparklines with fastcore: . from fastcore.utils import sparkline data = [9,6,None,1,4,0,8,15,10] print(f&#39;without &quot;empty_zero&quot;: {sparkline(data, empty_zero=False)}&#39;) print(f&#39; with &quot;empty_zero&quot;: {sparkline(data, empty_zero=True )}&#39;) . without &#34;empty_zero&#34;: ‚ñÖ‚ñÇ ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñá‚ñÖ with &#34;empty_zero&#34;: ‚ñÖ‚ñÇ ‚ñÅ‚ñÇ ‚ñÉ‚ñá‚ñÖ . For more information on this function, read the docs. Later in this post, we will describe how we used Rich to add color and animation to these sparklines. . fastcore EventTimer . Because we wanted streaming event data to automatically populate sparklines, we created EventTimer that constructs a histogram according to a frequency and time span you set. With EventTimer, you can add events with add, and get the number of events and their frequency: . from fastcore.utils import EventTimer from time import sleep import random def _randwait(): yield from (sleep(random.random()/200) for _ in range(100)) c = EventTimer(store=5, span=0.03) for o in _randwait(): c.add(1) print(f&#39;Num Events: {c.events}, Freq/sec: {c.freq:.01f}&#39;) print(&#39;Most recent: &#39;, sparkline(c.hist), *L(c.hist).map(&#39;{:.01f}&#39;)) . Num Events: 6, Freq/sec: 301.1 Most recent: ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÅ 323.6 274.8 291.3 390.9 283.6 . For more information, see the docs. . CLI Animations With Rich . Rich is an amazing python library that allows you to create beautiful, animated and interactive CLI interfaces. Below is a preview of some its features: . Rich also offers animated elements like spinners: . ... and progress bars: . While this post is not about rich, we highly recommend visiting the repo and the docs to learn more. Rich allows you to create your own custom elements. We created two custom elements - Stats and FixedPanel, which we describe below: . Stats: Sparklines with metrics . Stats renders a group of sparklines along with a spinner and a progress bar. First we define our sparklines, the last argument being a list of event types to count: . from ghtop.richext import * from ghtop.all_rich import * console = Console() s1 = ESpark(&#39;Issues&#39;, &#39;green&#39;, [IssueCommentEvent, IssuesEvent]) s2 = ESpark(&#39;PR&#39;, &#39;red&#39;, [PullRequestEvent, PullRequestReviewCommentEvent, PullRequestReviewEvent]) s3 = ESpark(&#39;Follow&#39;, &#39;blue&#39;, [WatchEvent, StarEvent]) s4 = ESpark(&#39;Other&#39;, &#39;red&#39;) s = Stats([s1,s2,s3,s4], store=5, span=.1, stacked=True) console.print(s) . üåç Issues PR Follow Other Quota /min 0.0 0.0 0.0 0.0 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0% . You can add events to update counters and sparklines with add_events: . evts = load_sample_events() s.add_events(evts) console.print(s) . üåç Issues PR Follow Other Quota /min 11772 ‚ñÅ‚ñá 16546 ‚ñÅ‚ñá 5991 ‚ñÅ‚ñá 6484 ‚ñÅ ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0% . You can update the progress bar with the update_prog method: . s.update_prog(50) console.print(s) . üåç Issues PR Follow Other Quota /min 4076 ‚ñÅ‚ñá 5408 ‚ñÅ‚ñá 1834 ‚ñÅ‚ñá 5998 ‚ñÅ ‚îÅ‚îÅ‚îÅ‚ï∏‚îÅ‚îÅ‚îÅ 50% . Here is what the animated version looks like: . . FixedPanel: A panel with fixed height . A key aspect of ghtop is showing events in different panels. We created FixedPanel to allow us to arrange panels in a grid that we can incrementally add events to: . p = FixedPanel(15, box=box.HORIZONTALS, title=&#39;ghtop&#39;) for e in evts: p.append(e) grid([[p,p]]) . ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ghtop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ghtop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üì™ dependabo‚Ä¶closed PR #3 o‚Ä¶herzli‚Ä¶&quot;Bump ‚Ä¶ üì™ dependabo‚Ä¶closed PR #3 ‚Ä¶herzli‚Ä¶&quot;Bump ‚Ä¶ ‚≠ê dongjun13 pushed 1 commi‚Ä¶dongjun13/2 ‚≠ê dongjun13 pushed 1 comm‚Ä¶dongjun13/2 ‚≠ê admmonito‚Ä¶pushed 1 commi‚Ä¶admmonitors/t‚Ä¶ ‚≠ê admmonito‚Ä¶pushed 1 comm‚Ä¶admmonitors/t‚Ä¶ ‚≠ê randomper‚Ä¶pushed 1 commi‚Ä¶randomperson1‚Ä¶ ‚≠ê randomper‚Ä¶pushed 1 comm‚Ä¶randomperson1‚Ä¶ ‚≠ê ahocevar pushed 6 commi‚Ä¶openlayers/ope‚Ä¶ ‚≠ê ahocevar pushed 6 commi‚Ä¶openlayers/op‚Ä¶ üè≠ arjmoto created branch ‚Ä¶arjmoto/redux-‚Ä¶ üè≠ arjmoto created branch‚Ä¶arjmoto/redux-‚Ä¶ üí¨ stale[bot‚Ä¶created commen‚Ä¶ironha‚Ä¶&quot;This ‚Ä¶ üí¨ stale[bot‚Ä¶created comme‚Ä¶ironha‚Ä¶&quot;This ‚Ä¶ ‚≠ê commit-b0‚Ä¶pushed 1 commi‚Ä¶commit-b0t/co‚Ä¶ ‚≠ê commit-b0‚Ä¶pushed 1 comm‚Ä¶commit-b0t/co‚Ä¶ ‚≠ê yakirgot pushed 2 commi‚Ä¶yakirgot/snake ‚≠ê yakirgot pushed 2 commi‚Ä¶yakirgot/snake üí¨ awolf78 created comment‚Ä¶Impulse‚Ä¶&quot;If yo‚Ä¶ üí¨ awolf78 created commen‚Ä¶Impulse‚Ä¶&quot;If yo‚Ä¶ ‚≠ê kreus7 pushed 1 commit‚Ä¶kreus7/kreusada‚Ä¶ ‚≠ê kreus7 pushed 1 commit‚Ä¶kreus7/kreusad‚Ä¶ ‚≠ê rgripper pushed 1 commi‚Ä¶rgripper/webco‚Ä¶ ‚≠ê rgripper pushed 1 commi‚Ä¶rgripper/webc‚Ä¶ üëÄ thelittle‚Ä¶started watchi‚Ä¶ritchie46/pol‚Ä¶ üëÄ thelittle‚Ä¶started watch‚Ä¶ritchie46/pol‚Ä¶ üè≠ adrian698 created branch‚Ä¶adrian698/Test üè≠ adrian698 created branc‚Ä¶adrian698/Test ‚≠ê mergify[b‚Ä¶pushed 2 commi‚Ä¶spbu-coding/6‚Ä¶ ‚≠ê mergify[b‚Ä¶pushed 2 comm‚Ä¶spbu-coding/6‚Ä¶ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ . To learn more about our extensions to rich see these docs. . A demo of ghtop animations . Putting all of this together, we get the following results: . 4 Panels with a sparkline for different types of events: . . single panel with a sparkline . . To learn more about ghtop, see the docs. . Interesting python features used . While making these docs, we used the following python features that at least one person we demoed it to found interesting or didn&#39;t know about. If you have been using python for sometime, you might know about all or most of these features: . yield from . Generators are a powerful feature of python, which are especially useful for iterating through large datasets lazily. . dequeue . f-strings .",
            "url": "https://librecv.github.io/blog/ghtop",
            "relUrl": "/ghtop",
            "date": " ‚Ä¢ Jan 29, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://librecv.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Learn more about us in www.librecv.org . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://librecv.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://librecv.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}