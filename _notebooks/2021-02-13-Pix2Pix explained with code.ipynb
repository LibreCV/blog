{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXACY0cbJsig"
   },
   "source": [
    "# Pix2Pix - Image to image translation with Conditional Adversarial Networks\n",
    "> A tutorial on Pix2Pix Conditional GANs and implementation with PyTorch\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [gans, pytorch]\n",
    "- author: \"<a href='https://linktr.ee/AniketMaurya'>Aniket Maurya</a>\"\n",
    "- image: https://ik.imagekit.io/aniket/blog/pix2pix/overview_eTCam8Vc0?tr=w-1200,h-628,fo-auto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zRzzQbeJsim"
   },
   "source": [
    "## Author Introduction\n",
    "> Hi! My name is Aniket Maurya. I am a Machine Learning Engineer at Quinbay Technologies, India. I research and build ML products. I like to share my limited knowledge of Machine Learning and Deep Learning with on my [blog](https://blog.aniketmaurya.ml) or [YouTube channel](https://studio.youtube.com/channel/UCRuFsj94hWecPkuEr4f5Xww).\n",
    "You can connect with me on [Linkedin](https://linkedin.com/in/aniketmaurya)/[Twitter](https://twitter.com/aniketmaurya)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2aU8_EtJsim"
   },
   "source": [
    "## Introduction to Conditional Adversarial Networks\n",
    "\n",
    "![Pix2Pix example 01](https://ik.imagekit.io/aniket/blog/pix2pix/pix2pix-training_4eZEIv0Mo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxK9HG0uJsin"
   },
   "source": [
    "Image to Image translation means transforming the given source image into a different image. Gray scale image to colour image conversion is one such example of image of image translation. \n",
    "\n",
    "In this tutorial we will discuss [GANs](https://arxiv.org/abs/1406.2661), a few points from Pix2Pix paper and implement the Pix2Pix network to translate segmented facade into real pictures.\n",
    "We will create the Pix2Pix model in PyTorch and use PyTorch lightning to avoid boilerplates.\n",
    "\n",
    "> Authors of [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) paper has also made the source code publically available on [GitHub](https://github.com/phillipi/pix2pix).\n",
    "\n",
    "> A more detailed tutorial on GANs can be found here - [Yann LeCunâ€™s Deep Learning Course at CDS](https://atcold.github.io/pytorch-Deep-Learning/en/week09/09-3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYtoG1h7Jsin"
   },
   "source": [
    "GANs are Generative models that learns a mapping from random noise vector (z) to an output image.\n",
    "`G(z) -> Image (y)`\n",
    "\n",
    "For example, GANs can learn mapping from random normal vectors to generate smiley images. For training such a GAN we just need a set of smiley images and train the GAN with an [adversarial loss](https://www.quora.com/What-is-adversarial-loss-in-machine-learning) ðŸ™‚. After the model is trained we can use random normal noise vectors to generate images that were not in the training dataset.\n",
    "\n",
    "But what if we want to build a network in such a way that we can control what the model will generate. In our case we want the model to generate a laughing smiley.\n",
    "\n",
    "Conditional GANs are Generative networks which learn mapping from random noise vectors and a conditional vector to output an image.\n",
    "Suppose we have 4 types of smileys - smile, laugh, sad and angry (ðŸ™‚ ðŸ˜‚ ðŸ˜” ðŸ˜¡). So our class vector for smile ðŸ™‚ can be `(1,0,0,0)`, laugh can be ðŸ˜‚ `(0,1,0,0)` and similarly for others.\n",
    "Here the conditional vector is the smiley embedding.\n",
    "\n",
    "During training of the generator the conditional image is passed to the generator and fake image is generated. The fake image is then passed through the discriminator along with the conditional image, both fake image and conditional image are concatenated. Discriminator penalizes the generator if it correctly classifies the fake image as fake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTVi7ku8Jsin"
   },
   "source": [
    "## Pix2Pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EXOoEw5DJsio"
   },
   "source": [
    "**Pix2Pix** is an image-to-image translation Generative Adversarial Networks that learns a mapping from an image X and a random noise Z to output image Y or in simple language it learns to translate the source image into a different distribution of image.\n",
    "\n",
    "During the time Pix2Pix was released, several other works were also using Conditional GANs on discrete labels. Pix2Pix uses a [U-Net](https://arxiv.org/abs/1505.04597) based architecture for the Generator and for the Discriminator a `PatchGAN Classifier` is used.\n",
    "\n",
    "![Pix2Pix Generator arch](https://ik.imagekit.io/aniket/blog/pix2pix/pix2pix_Unet_arch_Oen99KmZw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39#issuecomment-305575964) is what [Phillipi](https://github.com/phillipi) has to say about PatchGAN - \n",
    "![pix2pix author patchgan](https://ik.imagekit.io/aniket/blog/pix2pix/patchgan_R2hWYn2z0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg7BW1PtJsio"
   },
   "source": [
    "Pix2Pix Generator is an U-Net based architecture which is an encoder-decoder network with skip connections. The name U-Net highlights the structure of the \"U\" shaped network. Both generator and discriminator uses **Convolution-BatchNorm-ReLu** like module or in simple words we can say that it is the unit block of the generator and discriminator.\n",
    "Skip connections are added between each layer i and layer `nâˆ’i`, where n is the total number of layers. At each skip connection all the channels from current layer i are concatenated with all the channels at `n-i` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl5eszhGJsio"
   },
   "source": [
    "Lets understand it more with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r8dgpQOKEKw"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "!pip install -q pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoMVnYJWJsip"
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zLutwquJsip"
   },
   "source": [
    "Uncomment the below code to download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y__s3_uoJsip"
   },
   "outputs": [],
   "source": [
    "# !wget http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/facades.tar.gz\n",
    "# !tar -xvf facades.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPtaB7zeJsiq"
   },
   "source": [
    "After downloading the dataset we create Dataloader which loads our conditional and real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgBXK0fUJsiq"
   },
   "outputs": [],
   "source": [
    "path = \"./facades/train/\"\n",
    "\n",
    "\n",
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, path, target_size=None):\n",
    "        self.filenames = glob(str(Path(path) / \"*\"))\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        image = Image.open(filename)\n",
    "        image = transforms.functional.to_tensor(image)\n",
    "        image_width = image.shape[2]\n",
    "\n",
    "        real = image[:, :, : image_width // 2]\n",
    "        condition = image[:, :, image_width // 2 :]\n",
    "\n",
    "        target_size = self.target_size\n",
    "        if target_size:\n",
    "            condition = nn.functional.interpolate(condition, size=target_size)\n",
    "            real = nn.functional.interpolate(real, size=target_size)\n",
    "\n",
    "        return real, condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGIj6zEnJsir"
   },
   "source": [
    "In the first part of U-Net shaped network the layer size decreases, we create a `DownSampleConv` module for this. This module will contain the unit block that we just created `ConvBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXFHGMrCJsir"
   },
   "outputs": [],
   "source": [
    "class DownSampleConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True):\n",
    "        \"\"\"\n",
    "        Paper details:\n",
    "        - C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        - All convolutions are 4Ã—4 spatial filters applied with stride 2\n",
    "        - Convolutions in the encoder downsample by a factor of 2\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "        if self.activation:\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0CXTHD4Jsir"
   },
   "source": [
    "Now in the second part the network expands and so we create `UpSampleConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1NkBrkPJsir"
   },
   "outputs": [],
   "source": [
    "class UpSampleConv(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel=4,\n",
    "        strides=2,\n",
    "        padding=1,\n",
    "        activation=True,\n",
    "        batchnorm=True,\n",
    "        dropout=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.batchnorm = batchnorm\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n",
    "\n",
    "        if batchnorm:\n",
    "            self.bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        if activation:\n",
    "            self.act = nn.ReLU(True)\n",
    "\n",
    "        if dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.deconv(x)\n",
    "        if self.batchnorm:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFlkpsfGJsis"
   },
   "source": [
    "Now the basic blocks of the Pix2Pix generated is created, we create the generator module. Generator is formed of expanding and contracting layers.\n",
    "The first part network contracts and then expands again, i.e. first we have encoder block and then decoder block.\n",
    "Below is the encoder-decoder of U-Net network configuration from official paper. Here `C` denotes the unit block that we created `ConvBlock` and `D` denotes `Drop Out` with value _0.5_.\n",
    "In the decoder, the output tensors from `n-i` layer of encoder concatenates with `i` layer of the decoder. Also the first three blocks of the decoder has drop out layers.\n",
    "\n",
    "```\n",
    "Encoder:  C64-C128-C256-C512-C512-C512-C512-C512\n",
    "\n",
    "Decoder:  CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4quKiOsNJsit"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Paper details:\n",
    "        - Encoder: C64-C128-C256-C512-C512-C512-C512-C512\n",
    "        - All convolutions are 4Ã—4 spatial filters applied with stride 2\n",
    "        - Convolutions in the encoder downsample by a factor of 2\n",
    "        - Decoder: CD512-CD1024-CD1024-C1024-C1024-C512 -C256-C128\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder/donwsample convs\n",
    "        self.encoders = [\n",
    "            DownSampleConv(in_channels, 64, batchnorm=False),  # bs x 64 x 128 x 128\n",
    "            DownSampleConv(64, 128),  # bs x 128 x 64 x 64\n",
    "            DownSampleConv(128, 256),  # bs x 256 x 32 x 32\n",
    "            DownSampleConv(256, 512),  # bs x 512 x 16 x 16\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 8 x 8\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 4 x 4\n",
    "            DownSampleConv(512, 512),  # bs x 512 x 2 x 2\n",
    "            DownSampleConv(512, 512, batchnorm=False),  # bs x 512 x 1 x 1\n",
    "        ]\n",
    "\n",
    "        # decoder/upsample convs\n",
    "        self.decoders = [\n",
    "            UpSampleConv(512, 512, dropout=True),  # bs x 512 x 2 x 2\n",
    "            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 4 x 4\n",
    "            UpSampleConv(1024, 512, dropout=True),  # bs x 512 x 8 x 8\n",
    "            UpSampleConv(1024, 512),  # bs x 512 x 16 x 16\n",
    "            UpSampleConv(1024, 256),  # bs x 256 x 32 x 32\n",
    "            UpSampleConv(512, 128),  # bs x 128 x 64 x 64\n",
    "            UpSampleConv(256, 64),  # bs x 64 x 128 x 128\n",
    "        ]\n",
    "        self.decoder_channels = [512, 512, 512, 512, 256, 128, 64]\n",
    "        self.final_conv = nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips_cons = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "\n",
    "            skips_cons.append(x)\n",
    "\n",
    "        skips_cons = list(reversed(skips_cons[:-1]))\n",
    "        decoders = self.decoders[:-1]\n",
    "\n",
    "        for decoder, skip in zip(decoders, skips_cons):\n",
    "            x = decoder(x)\n",
    "            # print(x.shape, skip.shape)\n",
    "            x = torch.cat((x, skip), axis=1)\n",
    "\n",
    "        x = self.decoders[-1](x)\n",
    "        # print(x.shape)\n",
    "        x = self.final_conv(x)\n",
    "        return self.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(torch.randn(1, 3, 256, 256)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lLGrACoJsit"
   },
   "source": [
    "### Discriminator\n",
    "A discriminator is a ConvNet which learns to classify images into discrete labels. In GANs, discriminators learns to predict whether the given image is real or fake.\n",
    "PatchGAN is the discriminator used for Pix2Pix. Its architecture is different from a typical image classification ConvNet because of the output layer size. In convnets output layer size is equal to the number of classes while in PatchGAN output layer size is a 2D matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7KgnW06Jsiu"
   },
   "source": [
    "Now we create our Discriminator - **PatchGAN**. In this network we use the same `DownSampleConv` module that we created for generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtCmdjJeJsiu"
   },
   "outputs": [],
   "source": [
    "class PatchGAN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.d1 = DownSampleConv(input_channels, 64, batchnorm=False)\n",
    "        self.d2 = DownSampleConv(64, 128)\n",
    "        self.d3 = DownSampleConv(128, 256)\n",
    "        self.d4 = DownSampleConv(256, 512)\n",
    "        self.final = nn.Conv2d(512, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "        x0 = self.d1(x)\n",
    "        x1 = self.d2(x0)\n",
    "        x2 = self.d3(x1)\n",
    "        x3 = self.d4(x2)\n",
    "        xn = self.final(x3)\n",
    "        return xn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYm1jdg0Jsiu"
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- resource for BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4vPQ_qlJsiu"
   },
   "source": [
    "Loss function used in Pix2Pix are `Adversarial loss` and  `Reconstruction loss`. Adversarial loss is used to penalize the generator to predict more realistic images. In conditional GANs, generators job is not only to produce realistic image but also to be near the ground truth output. Reconstruction Loss helps network to produce the realistic image near the conditional image.\n",
    "\n",
    "\n",
    "```python\n",
    "adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "reconstruction_loss = nn.L1Loss()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWxi6K7EJsiv"
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "# https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "\n",
    "def _weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "def display_progress(cond, fake, real, figsize=(10,5)):\n",
    "    cond = cond.detach().cpu().permute(1, 2, 0)\n",
    "    fake = fake.detach().cpu().permute(1, 2, 0)\n",
    "    real = real.detach().cpu().permute(1, 2, 0)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=figsize)\n",
    "    ax[0].imshow(cond)\n",
    "    ax[2].imshow(fake)\n",
    "    ax[1].imshow(real)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5mTw8-iJsiv"
   },
   "outputs": [],
   "source": [
    "class Pix2Pix(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, display_step=25):\n",
    "\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.display_step = display_step\n",
    "        self.gen = Generator(in_channels, out_channels)\n",
    "        self.patch_gan = PatchGAN(in_channels + out_channels)\n",
    "\n",
    "        # intializing weights\n",
    "        self.gen = self.gen.apply(_weights_init)\n",
    "        self.patch_gan = self.patch_gan.apply(_weights_init)\n",
    "\n",
    "        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.recon_criterion = nn.L1Loss()\n",
    "\n",
    "    def _gen_step(self, real_images, conditioned_images):\n",
    "        # Pix2Pix has adversarial and a reconstruction loss\n",
    "        # First calculate the adversarial loss\n",
    "        fake_images = self.gen(conditioned_images)\n",
    "        disc_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        recon_loss = self.recon_criterion(fake_images, real_images)\n",
    "        lambda_recon = self.hparams.lambda_recon\n",
    "\n",
    "        return adversarial_loss + lambda_recon * recon_loss\n",
    "\n",
    "    def _disc_step(self, real_images, conditioned_images):\n",
    "        fake_images = self.gen(conditioned_images).detach()\n",
    "        fake_logits = self.patch_gan(fake_images, conditioned_images)\n",
    "\n",
    "        real_logits = self.patch_gan(real_images, conditioned_images)\n",
    "\n",
    "        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n",
    "        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.learning_rate\n",
    "        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=lr)\n",
    "        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=lr)\n",
    "        return disc_opt, gen_opt\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch\n",
    "\n",
    "        loss = None\n",
    "        if optimizer_idx == 0:\n",
    "            loss = self._disc_step(real, condition)\n",
    "            self.log('PatchGAN Loss', loss)\n",
    "        elif optimizer_idx == 1:\n",
    "            loss = self._gen_step(real, condition)\n",
    "            self.log('Generator Loss', loss)\n",
    "        \n",
    "        if self.current_epoch%self.display_step==0 and batch_idx==0 and optimizer_idx==1:\n",
    "            fake = self.gen(condition).detach()\n",
    "            display_progress(condition[0], fake[0], real[0])\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0m0VwD6Jsiv"
   },
   "source": [
    "Now that the network is implemented now we are ready to train. You can also modify the dataloader and train on custom dataset.\n",
    "\n",
    "Hope you liked the article! Happy training ðŸ˜ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7XnMNwUJsiw"
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "# These configurations are from paper\n",
    "\n",
    "adv_criterion = nn.BCEWithLogitsLoss() \n",
    "recon_criterion = nn.L1Loss() \n",
    "lambda_recon = 200\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "display_step = 200\n",
    "batch_size = 4\n",
    "lr = 0.0002\n",
    "target_size = 256\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b0d1395947ae40bab4e06d6ab8e72dc3",
      "6e994fe965be4bf7a8926f44a86f6a73",
      "b5ca181e2178491aa6b6a86fcbdfc56e",
      "f78a678993c3490a96c9a9d6b02279a6",
      "0afbe34dad9c429d9d7d58df972d3621",
      "916a5759f1b74f4a83fa7c2f1e71f9a0",
      "f342e152d80844e0bb7c59e998f62953",
      "af655c396e0a4929b4b5c35790723df5"
     ]
    },
    "id": "irlp--WuJsiw",
    "outputId": "1e4032d8-6651-4a89-e8a4-70916cd902fc"
   },
   "outputs": [],
   "source": [
    "dataset = FacadesDataset(path, target_size=target_size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "pix2pix = Pix2Pix(3, 3)\n",
    "trainer = pl.Trainer(max_epochs=1000, gpus=1)\n",
    "trainer.fit(pix2pix, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D9EYUsYJsiw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2021-02-13-Pix2Pix explained with code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0afbe34dad9c429d9d7d58df972d3621": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6e994fe965be4bf7a8926f44a86f6a73": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "916a5759f1b74f4a83fa7c2f1e71f9a0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af655c396e0a4929b4b5c35790723df5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0d1395947ae40bab4e06d6ab8e72dc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5ca181e2178491aa6b6a86fcbdfc56e",
       "IPY_MODEL_f78a678993c3490a96c9a9d6b02279a6"
      ],
      "layout": "IPY_MODEL_6e994fe965be4bf7a8926f44a86f6a73"
     }
    },
    "b5ca181e2178491aa6b6a86fcbdfc56e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "Epoch 65:  89%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_916a5759f1b74f4a83fa7c2f1e71f9a0",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0afbe34dad9c429d9d7d58df972d3621",
      "value": 89
     }
    },
    "f342e152d80844e0bb7c59e998f62953": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f78a678993c3490a96c9a9d6b02279a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af655c396e0a4929b4b5c35790723df5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f342e152d80844e0bb7c59e998f62953",
      "value": " 89/100 [00:38&lt;00:04,  2.32it/s, loss=16.5, v_num=3]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
